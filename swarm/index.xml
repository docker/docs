<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Swarms on Docker Docs</title>
    <link>http://docs-stage.docker.com/swarm/</link>
    <description>Recent content in Swarms on Docker Docs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://docs-stage.docker.com/swarm/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>API response codes</title>
      <link>http://docs-stage.docker.com/swarm/status-code-comparison-to-docker/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/status-code-comparison-to-docker/</guid>
      <description>

&lt;h1 id=&#34;understand-the-swarm-vs-engine-response-codes&#34;&gt;Understand the Swarm vs. Engine response codes&lt;/h1&gt;

&lt;p&gt;Docker Engine provides a REST API for making calls to the Engine daemon. Docker Swarm allows a caller to make the same calls to a cluster of Engine daemons. While the API calls are the same, the API response status codes do differ. This document explains the differences.&lt;/p&gt;

&lt;p&gt;Four methods are included, and they are GET, POST, PUT and DELETE.&lt;/p&gt;

&lt;p&gt;The comparison is based on api v1.22, and all Docker Status Codes in api v1.22 are referenced from &lt;a href=&#34;https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.22.md&#34;&gt;docker-remote-api-v1.22&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get&#34;&gt;GET&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/_ping&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;ping&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/events&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getEvents&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/info&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getInfo&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/version&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getVersion&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/json&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getImagesJSON&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/viz&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;notImplementedHandler&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;501&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;no this api&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/search&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyRandom&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/get&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getImages&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/{name:.*}/get&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyImageGet&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/{name:.*}/history&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyImage&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/{name:.*}/json&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyImage&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/ps&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getContainersJSON&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;no this api&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;no this api&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;no this api&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/json&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getContainersJSON&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/archive&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/export&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/changes&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/json&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getContainerJSON&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/top&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/logs&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;101&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;101&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/stats&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/attach/ws&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyHijack&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/exec/{execid:.*}/json&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/networks&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getNetworks&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/networks/{networkid:.*}&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getNetwork&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/volumes&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getVolumes&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/volumes/{volumename:.*}&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;getVolume&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;post&#34;&gt;POST&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/auth&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyRandom&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/commit&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postCommit&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/build&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postBuild&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/create&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postImagesCreate&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/load&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postImagesLoad&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/{name:.*}/push&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyImagePush&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/{name:.*}/tag&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postTagImage&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;409&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/create&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postContainersCreate&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;406&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;409&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/kill&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainerAndForceRefresh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/pause&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainerAndForceRefresh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/unpause&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainerAndForceRefresh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/rename&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postRenameContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;409&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;409&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/restart&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainerAndForceRefresh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/start&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postContainersStart&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;304&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/stop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainerAndForceRefresh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;304&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;304&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/update&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainerAndForceRefresh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/wait&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainerAndForceRefresh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/resize&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/attach&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyHijack&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;101&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;101&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/copy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/exec&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postContainersExec&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;409&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/exec/{execid:.*}/start&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postExecStart&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;409&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;409&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/exec/{execid:.*}/resize&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/networks/create&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postNetworksCreate&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/networks/{networkid:.*}/connect&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyNetworkConnect&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/networks/{networkid:.*}/disconnect&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyNetworkDisconnect&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/volumes/create&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;postVolumesCreate&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;201&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;put&#34;&gt;PUT&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}/archive&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;proxyContainer&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;403&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;403&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;delete&#34;&gt;DELETE&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/containers/{name:.*}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;deleteContainers&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;400&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/images/{name:.*}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;deleteImages&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;409&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/networks/{networkid:.*}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;deleteNetworks&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;200&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Route:          &lt;code&gt;/volumes/{name:.*}&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Handler:        &lt;code&gt;deleteVolumes&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Swarm Status Code&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Docker Status Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;204&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;404&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;409&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Build a Swarm cluster for production</title>
      <link>http://docs-stage.docker.com/swarm/install-manual/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/install-manual/</guid>
      <description>

&lt;h1 id=&#34;build-a-swarm-cluster-for-production&#34;&gt;Build a Swarm cluster for production&lt;/h1&gt;

&lt;p&gt;This page teaches you to deploy a high-availability Docker Swarm cluster.
Although the example installation uses the Amazon Web Services (AWS) platform,
you can deploy an equivalent Docker Swarm cluster on many other platforms. In this example, you do the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#prerequisites&#34;&gt;Verify you have the prequisites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-1-add-network-security-rules&#34;&gt;Establish basic network security&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2-create-your-instances&#34;&gt;Create your nodes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3-install-engine-on-each-node&#34;&gt;Install Engine on each node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4-set-up-a-discovery-backend&#34;&gt;Configure a discovery backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5-create-swarm-cluster&#34;&gt;Create Swarm cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6-communicate-with-the-swarm&#34;&gt;Communicate with the Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-7-test-swarm-failover&#34;&gt;Test the high-availability Swarm managers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#additional-resources&#34;&gt;Additional Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a gentler introduction to Swarm, try the &lt;a href=&#34;../swarm/install-w-machine/&#34;&gt;Evaluate Swarm in a sandbox&lt;/a&gt; page.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;An Amazon Web Services (AWS) account&lt;/li&gt;
&lt;li&gt;Familiarity with AWS features and tools, such as:

&lt;ul&gt;
&lt;li&gt;Elastic Cloud (EC2) Dashboard&lt;/li&gt;
&lt;li&gt;Virtual Private Cloud (VPC) Dashboard&lt;/li&gt;
&lt;li&gt;VPC Security groups&lt;/li&gt;
&lt;li&gt;Connecting to an EC2 instance using SSH&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;step-1-add-network-security-rules&#34;&gt;Step 1. Add network security rules&lt;/h2&gt;

&lt;p&gt;AWS uses a &amp;ldquo;security group&amp;rdquo; to allow specific types of network traffic on your
VPC network. The &lt;strong&gt;default&lt;/strong&gt; security group&amp;rsquo;s initial set of rules deny all
inbound traffic, allow all outbound traffic, and allow all traffic between
instances.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;re  going to add a couple of rules to allow inbound SSH connections and
inbound container images. This set of rules somewhat protects the Engine, Swarm,
and Consul ports. For a production environment, you would apply more restrictive
security measures. Do not leave Docker Engine ports unprotected.&lt;/p&gt;

&lt;p&gt;From your AWS home console, do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Click &lt;strong&gt;VPC - Isolated Cloud Resources&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The VPC Dashboard opens.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Navigate to &lt;strong&gt;Security Groups&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select the &lt;strong&gt;default&lt;/strong&gt; security group that&amp;rsquo;s associated with your default VPC.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Add the following two rules.&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;
  &lt;th&gt;Type&lt;/th&gt;
  &lt;th&gt;Protocol&lt;/th&gt;
  &lt;th&gt;Port Range&lt;/th&gt;
  &lt;th&gt;Source&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;SSH&lt;/td&gt;
  &lt;td&gt;TCP&lt;/td&gt;
  &lt;td&gt;22&lt;/td&gt;
  &lt;td&gt;0.0.0.0/0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;HTTP&lt;/td&gt;
  &lt;td&gt;TCP&lt;/td&gt;
  &lt;td&gt;80&lt;/td&gt;
  &lt;td&gt;0.0.0.0/0&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The SSH connection allows you to connect to the host while the HTTP is for container images.&lt;/p&gt;

&lt;h2 id=&#34;step-2-create-your-instances&#34;&gt;Step 2. Create your instances&lt;/h2&gt;

&lt;p&gt;In this step, you create five Linux hosts that are part of your default security
group. When complete, the example deployment contains three types of nodes:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Node Description&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Swarm primary and secondary managers&lt;/td&gt;
&lt;td&gt;&lt;code&gt;manager0&lt;/code&gt;,  &lt;code&gt;manager1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Swarm node&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node0&lt;/code&gt;, &lt;code&gt;node1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Discovery backend&lt;/td&gt;
&lt;td&gt;&lt;code&gt;consul0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To create the instances do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open the EC2 Dashboard and launch four EC2 instances, one at a time.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;During &lt;strong&gt;Step 1: Choose an Amazon Machine Image (AMI)&lt;/strong&gt;, pick the &lt;em&gt;Amazon Linux AMI&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;During &lt;strong&gt;Step 5: Tag Instance&lt;/strong&gt;, under &lt;strong&gt;Value&lt;/strong&gt;, give each instance one of these names:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;manager0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;manager1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;consul0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;During &lt;strong&gt;Step 6: Configure Security Group&lt;/strong&gt;, choose &lt;strong&gt;Select an existing security group&lt;/strong&gt; and pick the &amp;ldquo;default&amp;rdquo; security group.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Review and launch your instances.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-3-install-engine-on-each-node&#34;&gt;Step 3. Install Engine on each node&lt;/h2&gt;

&lt;p&gt;In this step, you install Docker Engine on each node. By installing Engine, you enable the Swarm manager to address the nodes via the Engine CLI and API.&lt;/p&gt;

&lt;p&gt;SSH to each node in turn and do the following.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Update the yum packages.&lt;/p&gt;

&lt;p&gt;Keep an eye out for the &amp;ldquo;y/n/abort&amp;rdquo; prompt:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo yum update
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run the installation script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -sSL https://get.docker.com/ | sh
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Configure and start Engine so it listens for Swarm nodes on port &lt;code&gt;2375&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify that Docker Engine is installed correctly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run hello-world
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output should display a &amp;ldquo;Hello World&amp;rdquo; message and other text without any
error messages.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Give the &lt;code&gt;ec2-user&lt;/code&gt; root privileges:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo usermod -aG docker ec2-user
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Enter &lt;code&gt;logout&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;troubleshooting&#34;&gt;Troubleshooting&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If entering a &lt;code&gt;docker&lt;/code&gt; command produces a message asking whether docker is
available on this host, it may be because the user doesn&amp;rsquo;t have root privileges.
If so, use &lt;code&gt;sudo&lt;/code&gt; or give the user root privileges.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For this example, don&amp;rsquo;t create an AMI image from one of your instances running
Docker Engine and then re-use it to create the other instances. Doing so will
produce errors.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If your host cannot reach Docker Hub, the &lt;code&gt;docker run&lt;/code&gt; commands that pull
container images may fail. In that case, check that your VPC is associated with
a security group with a rule that allows inbound traffic (e.g.,
HTTP/TCP/80/0.0.0.0/0). Also Check the &lt;a href=&#34;http://status.docker.com/&#34;&gt;Docker Hub status
page&lt;/a&gt; for service availability.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;step-4-set-up-a-discovery-backend&#34;&gt;Step 4. Set up a discovery backend&lt;/h2&gt;

&lt;p&gt;Here, you&amp;rsquo;re going to create a minimalist discovery backend. The Swarm managers
and nodes use this backend to authenticate themselves as members of the cluster.
The Swarm managers also use this information to identify which nodes are
available to run containers.&lt;/p&gt;

&lt;p&gt;To keep things simple, you are going to run a single consul daemon on the same
host as one of the Swarm managers.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;To start, copy the following launch command to a text file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use SSH to connect to the &lt;code&gt;manager0&lt;/code&gt; and &lt;code&gt;consul0&lt;/code&gt; instance.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ifconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;From the output, copy the &lt;code&gt;eth0&lt;/code&gt; IP address from &lt;code&gt;inet addr&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Using SSH, connect to the &lt;code&gt;manager0&lt;/code&gt; and &lt;code&gt;consul0&lt;/code&gt; instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Paste the launch command you created in step 1. into the command line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Your Consul node is up and running, providing your cluster with a discovery
backend. To increase its reliability, you can create a high-availability cluster
using a trio of consul nodes using the link mentioned at the end of this page.
(Before creating a cluster of consul nodes, update the VPC security group with
rules to allow inbound traffic on the required port numbers.)&lt;/p&gt;

&lt;h2 id=&#34;step-5-create-swarm-cluster&#34;&gt;Step 5. Create Swarm cluster&lt;/h2&gt;

&lt;p&gt;After creating the discovery backend, you can create the Swarm managers. In this step, you are going to create two Swarm managers in a high-availability configuration. The first manager you run becomes the Swarm&amp;rsquo;s &lt;em&gt;primary manager&lt;/em&gt;. Some documentation still refers to a primary manager as a &amp;ldquo;master&amp;rdquo;, but that term has been superseded. The second manager you run serves as a &lt;em&gt;replica&lt;/em&gt;. If the primary manager becomes unavailable, the cluster elects the replica as the primary manager.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;To create the primary manager in a high-availability Swarm cluster, use the following syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &amp;lt;manager0_ip&amp;gt;:4000 consul://&amp;lt;consul_ip&amp;gt;:8500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because this is particular manager is on the same &lt;code&gt;manager0&lt;/code&gt; and &lt;code&gt;consul0&lt;/code&gt;
instance as the consul node, replace both &lt;code&gt;&amp;lt;manager0_ip&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;consul_ip&amp;gt;&lt;/code&gt;
with the same IP address. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise 172.30.0.161:4000 consul://172.30.0.161:8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Enter &lt;code&gt;docker ps&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;From the output, verify that both a Swarm cluster and a consul container are running.
Then, disconnect from the &lt;code&gt;manager0&lt;/code&gt; and &lt;code&gt;consul0&lt;/code&gt; instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connect to the &lt;code&gt;manager1&lt;/code&gt; node and use &lt;code&gt;ifconfig&lt;/code&gt; to get its IP address.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ifconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the secondary Swarm manager using following command.&lt;/p&gt;

&lt;p&gt;Replacing &lt;code&gt;&amp;lt;manager1_ip&amp;gt;&lt;/code&gt; with the IP address from the previous command, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &amp;lt;manager1_ip&amp;gt;:4000 consul://172.30.0.161:8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Enter &lt;code&gt;docker ps&lt;/code&gt;to verify that a Swarm container is running.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connect to &lt;code&gt;node0&lt;/code&gt; and &lt;code&gt;node1&lt;/code&gt; in turn and join them to the cluster.&lt;/p&gt;

&lt;p&gt;a. Get the node IP addresses with the &lt;code&gt;ifconfig&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;b. Start a Swarm container each using the following syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d swarm join --advertise=&amp;lt;node_ip&amp;gt;:2375 consul://&amp;lt;consul_ip&amp;gt;:8500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d swarm join --advertise=172.30.0.69:2375 consul://172.30.0.161:8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Your small Swarm cluster is up and running on multiple hosts, providing you with a high-availability virtual Docker Engine. To increase its reliability and capacity, you can add more Swarm managers, nodes, and a high-availability discovery backend.&lt;/p&gt;

&lt;h2 id=&#34;step-6-communicate-with-the-swarm&#34;&gt;Step 6. Communicate with the Swarm&lt;/h2&gt;

&lt;p&gt;You can communicate with the Swarm to get information about the managers and
nodes using the Swarm API, which is nearly the same as the standard Docker API.
In this example, you use SSL to connect to &lt;code&gt;manager0&lt;/code&gt; and &lt;code&gt;consul0&lt;/code&gt; host again.
Then, you address commands to the Swarm manager.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Get information about the manager and nodes in the cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H :4000 info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output gives the manager&amp;rsquo;s role as primary (&lt;code&gt;Role: primary&lt;/code&gt;) and
information about each of the nodes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run an application on the Swarm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H :4000 run hello-world
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check which Swarm node ran the application:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H :4000 ps
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-7-test-swarm-failover&#34;&gt;Step 7. Test Swarm failover&lt;/h2&gt;

&lt;p&gt;To see the replica instance take over, you&amp;rsquo;re going to shut down the primary
manager. Doing so kicks off an election, and the replica becomes the primary
manager. When you start the manager you shut down earlier, it becomes the
replica.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;SSH connection to the &lt;code&gt;manager0&lt;/code&gt; instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get the container id or name of the &lt;code&gt;swarm&lt;/code&gt; container:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Shut down the primary manager, replacing &lt;code&gt;&amp;lt;id_name&amp;gt;&lt;/code&gt; with the container&amp;rsquo;s id or name (for example, &amp;ldquo;8862717fe6d3&amp;rdquo; or &amp;ldquo;trusting_lamarr&amp;rdquo;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker rm -f &amp;lt;id_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the Swarm manager. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise 172.30.0.161:4000 consul://172.30.0.161:8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Review the Engine&amp;rsquo;s daemon logs the logs, replacing &lt;code&gt;&amp;lt;id_name&amp;gt;&lt;/code&gt; with the new container&amp;rsquo;s id or name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker logs &amp;lt;id_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output shows will show two entries like these ones:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time=&amp;quot;2016-02-02T02:12:32Z&amp;quot; level=info msg=&amp;quot;Leader Election: Cluster leadership lost&amp;quot;
time=&amp;quot;2016-02-02T02:12:32Z&amp;quot; level=info msg=&amp;quot;New leader elected: 172.30.0.160:4000&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To get information about the manager and nodes in the cluster, enter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H :4000 info
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can connect to the &lt;code&gt;manager1&lt;/code&gt; node and run the &lt;code&gt;info&lt;/code&gt; and &lt;code&gt;logs&lt;/code&gt; commands.
They will display corresponding entries for the change in leadership.&lt;/p&gt;

&lt;h2 id=&#34;additional-resources&#34;&gt;Additional Resources&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.docker.com/engine/installation/cloud/cloud-ex-aws/&#34;&gt;Installing Docker Engine on a cloud provider&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/multi-manager-setup/&#34;&gt;High availability in Docker Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/discovery/&#34;&gt;Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/progrium/consul/&#34;&gt;High-availability cluster using a trio of consul nodes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/networking/&#34;&gt;Networking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Command line reference</title>
      <link>http://docs-stage.docker.com/swarm/reference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/reference/</guid>
      <description>

&lt;h1 id=&#34;docker-swarm-command-line-reference&#34;&gt;Docker Swarm command line reference&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/reference/swarm/&#34;&gt;swarm&lt;/a&gt;  Run a Swarm container on Docker Engine&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/reference/create/&#34;&gt;create&lt;/a&gt;  Create a discovery token&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/reference/list/&#34;&gt;list&lt;/a&gt;  List the nodes in a Docker cluster&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/reference/manage/&#34;&gt;manage&lt;/a&gt;  Create a Swarm manager&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/reference/join/&#34;&gt;join&lt;/a&gt;  Create a Swarm node&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/reference/help/&#34;&gt;help&lt;/a&gt;  See a list of Swarm commands, or help for one command&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Configure Docker Swarm for TLS</title>
      <link>http://docs-stage.docker.com/swarm/configure-tls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/configure-tls/</guid>
      <description>

&lt;h1 id=&#34;configure-docker-swarm-for-tls&#34;&gt;Configure Docker Swarm for TLS&lt;/h1&gt;

&lt;p&gt;In this procedure you create a two-node Swarm cluster, a Docker Engine CLI, a
Swarm Manager, and a Certificate Authority as shown below. All the Docker Engine
hosts (&lt;code&gt;client&lt;/code&gt;, &lt;code&gt;swarm&lt;/code&gt;, &lt;code&gt;node1&lt;/code&gt;, and &lt;code&gt;node2&lt;/code&gt;) have a copy of the
CA&amp;rsquo;s certificate as well as their own key-pair signed by the CA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/tls-1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You will complete the following steps in this procedure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-1-set-up-the-prerequisites&#34;&gt;Step 1: Set up the prerequisites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2-create-a-certificate-authority-ca-server&#34;&gt;Step 2: Create a Certificate Authority (CA) server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3-create-and-sign-keys&#34;&gt;Step 3: Create and sign keys&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4-install-the-keys&#34;&gt;Step 4: Install the keys&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5-configure-the-engine-daemon-for-tls&#34;&gt;Step 5: Configure the Engine daemon for TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6-create-a-swarm-cluster&#34;&gt;Step 6: Create a Swarm cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-7-create-the-swarm-manager-using-tls&#34;&gt;Step 7: Create the Swarm Manager using TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-8-test-the-swarm-manager-configuration&#34;&gt;Step 8: Test the Swarm manager configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-9-configure-the-engine-cli-to-use-tls&#34;&gt;Step 9: Configure the Engine CLI to use TLS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;before-you-begin&#34;&gt;Before you begin&lt;/h3&gt;

&lt;p&gt;The article includes steps to create your own CA using OpenSSL. This is similar
to operating your own internal corporate CA and PKI. However, this &lt;code&gt;must not&lt;/code&gt;
be used as a guide to building a production-worthy internal CA and PKI. These
steps are included for demonstration purposes only - so that readers without
access to an existing CA and set of certificates can follow along and configure
Docker Swarm to use TLS.&lt;/p&gt;

&lt;h2 id=&#34;step-1-set-up-the-prerequisites&#34;&gt;Step 1: Set up the prerequisites&lt;/h2&gt;

&lt;p&gt;To complete this procedure you must stand up 5 (five) Linux servers. These
servers can be any mix of physical and virtual servers; they may be on premises
or in the public cloud. The following table lists each server name and its purpose.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Server name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ca&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Acts as the Certificate Authority (CA) server.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;swarm&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Acts as the Swarm Manager.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Act as a Swarm node.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Act as a Swarm node.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Acts as a remote Docker Engine client&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Make sure that you have SSH access to all 5 servers and that they can communicate with each other using DNS name resolution. In particular:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Open TCP port 2376 between the Swarm Manager and Swarm nodes&lt;/li&gt;
&lt;li&gt;Open TCP port 3376 between the Docker Engine client and the Swarm Manager&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can choose different ports if these are already in use. This example assumes
you use these ports though.&lt;/p&gt;

&lt;p&gt;Each server must run an operating system compatible with Docker Engine. For
simplicity, the steps that follow assume all servers are running Ubuntu 14.04
LTS.&lt;/p&gt;

&lt;h2 id=&#34;step-2-create-a-certificate-authority-ca-server&#34;&gt;Step 2: Create a Certificate Authority (CA) server&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:If you already have access to a CA and certificates, and are comfortable working with them, you should skip this step and go to the next.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this step, you configure a Linux server as a CA. You use this CA to create
and sign keys. This step included so that readers without access to an existing
CA (external or corporate) and certificates can follow along and complete the
later steps that require installing and using certificates. It is &lt;code&gt;not&lt;/code&gt;
intended as a model for how to deploy production-worthy CA.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Logon to the terminal of your CA server and elevate to root.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo su
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a private key called &lt;code&gt;ca-priv-key.pem&lt;/code&gt; for the CA:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl genrsa -out ca-priv-key.pem 2048
Generating RSA private key, 2048 bit long modulus
...........................................................+++
.....+++
e is 65537 (0x10001)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a public key called &lt;code&gt;ca.pem&lt;/code&gt; for the CA.&lt;/p&gt;

&lt;p&gt;The public key is based on the private key created in the previous step.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl req -config /usr/lib/ssl/openssl.cnf -new -key ca-priv-key.pem -x509 -days 1825 -out ca.pem
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter &#39;.&#39;, the field will be left blank.
-----
Country Name (2 letter code) [AU]:US
&amp;lt;output truncated&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You have now configured a CA server with a public and private keypair. You can inspect the contents of each key. To inspect the private key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl rsa -in ca-priv-key.pem -noout -text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To inspect the public key (cert): `&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl x509 -in ca.pem -noout -text`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command shows the partial contents of the CA&amp;rsquo;s public key.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl x509 -in ca.pem -noout -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 17432010264024107661 (0xf1eaf0f9f41eca8d)
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: C=US, ST=CA, L=Sanfrancisco, O=Docker Inc
        Validity
            Not Before: Jan 16 18:28:12 2016 GMT
            Not After : Jan 13 18:28:12 2026 GMT
        Subject: C=US, ST=CA, L=San Francisco, O=Docker Inc
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:d1:fe:6e:55:d4:93:fc:c9:8a:04:07:2d:ba:f0:
                    55:97:c5:2c:f5:d7:1d:6a:9b:f0:f0:55:6c:5d:90:
&amp;lt;output truncated&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Later, you&amp;rsquo;ll use this to certificate to sign keys for other servers in the
infrastructure.&lt;/p&gt;

&lt;h2 id=&#34;step-3-create-and-sign-keys&#34;&gt;Step 3: Create and sign keys&lt;/h2&gt;

&lt;p&gt;Now that you have a working CA, you need to create key pairs for the Swarm
Manager, Swarm nodes, and remote Docker Engine client. The commands and process
to create key pairs is identical for all servers.  You&amp;rsquo;ll create the following keys:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;code&gt;ca-priv-key.pem&lt;/td&gt;
    &lt;td&gt;The CA&#39;s private key and must be kept secure. It is used later to sign new keys for the other nodes in the environment. Together with the &lt;code&gt;ca.pem&lt;/code&gt; file, this makes up the CA&#39;s key pair.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;code&gt;ca.pem&lt;/td&gt;
    &lt;td&gt;The CA&#39;s public key (also called certificate). This is installed on all nodes in the environment so that all nodes trust certificates signed by the CA. Together with the &lt;code&gt;ca-priv-key.pem&lt;/code&gt; file, this makes up the CA&#39;s key pair.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;code&gt;&lt;i&gt;node&lt;/i&gt;.csr&lt;/code&gt;&lt;/td&gt;
    &lt;td&gt;A certificate signing request (CSR). A CSR is effectively an application to the CA to create a new key pair for a particular node. The CA takes the information provided in the CSR and generates the public and private key pair for that node.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;code&gt;&lt;i&gt;node&lt;/i&gt;-priv.key&lt;/code&gt;&lt;/td&gt;
    &lt;td&gt;A private key signed by the CA. The node uses this key to authenticate itself with remote Docker Engines. Together with the &lt;code&gt;&lt;i&gt;node&lt;/i&gt;-cert.pem&lt;/code&gt; file, this makes up a node&#39;s key pair.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;code&gt;&lt;i&gt;node&lt;/i&gt;-cert.pem&lt;/code&gt;&lt;/td&gt;
    &lt;td&gt;A certificate signed by the CA. This is not used in this example. Together with the &lt;code&gt;&lt;i&gt;node&lt;/i&gt;-priv.key&lt;/code&gt; file, this makes up a node&#39;s key pair&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The commands below show how to create keys for all of your nodes. You perform this procedure in a working directory located on your CA server.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Logon to the terminal of your CA server and elevate to root.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo su
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a private key &lt;code&gt;swarm-priv-key.pem&lt;/code&gt; for your Swarm Manager&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl genrsa -out swarm-priv-key.pem 2048
Generating RSA private key, 2048 bit long modulus
............................................................+++
........+++
e is 65537 (0x10001)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Generate a certificate signing request (CSR) &lt;code&gt;swarm.csr&lt;/code&gt; using the private key you create in the previous step.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl req -subj &amp;quot;/CN=swarm&amp;quot; -new -key swarm-priv-key.pem -out swarm.csr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remember, this is only for demonstration purposes. The process to create a
CSR will be slightly different in real-world production environments.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create the certificate &lt;code&gt;swarm-cert.pem&lt;/code&gt; based on the CSR created in the previous step.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl x509 -req -days 1825 -in swarm.csr -CA ca.pem -CAkey ca-priv-key.pem -CAcreateserial -out swarm-cert.pem -extensions v3_req -extfile /usr/lib/ssl/openssl.cnf
&amp;lt;snip&amp;gt;
# openssl rsa -in swarm-priv-key.pem -out swarm-priv-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You now have a keypair for the Swarm Manager.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Repeat the steps above for the remaining nodes in your infrastructure (&lt;code&gt;node1&lt;/code&gt;, &lt;code&gt;node2&lt;/code&gt;, and &lt;code&gt;client&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Remember to replace the &lt;code&gt;swarm&lt;/code&gt; specific values with the values relevant to the node you are creating the key pair for.&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;Server name&lt;/th&gt;
&lt;th&gt;Private key&lt;/th&gt;
&lt;th&gt;CSR&lt;/th&gt;
&lt;th&gt;Certificate&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node1 &lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node1-priv-key.pem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node1.csr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node1-cert.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node2-priv-key.pem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node2.csr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node2-cert.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;client-priv-key.pem&lt;/td&gt;
&lt;td&gt;&lt;code&gt;client.csr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;client-cert.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify that your working directory contains the following files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ls -l
total 64
-rw-r--r-- 1 root   root   1679 Jan 16 18:27 ca-priv-key.pem
-rw-r--r-- 1 root   root   1229 Jan 16 18:28 ca.pem
-rw-r--r-- 1 root   root     17 Jan 18 09:56 ca.srl
-rw-r--r-- 1 root   root   1086 Jan 18 09:56 client-cert.pem
-rw-r--r-- 1 root   root    887 Jan 18 09:55 client.csr
-rw-r--r-- 1 root   root   1679 Jan 18 09:56 client-priv-key.pem
-rw-r--r-- 1 root   root   1082 Jan 18 09:44 node1-cert.pem
-rw-r--r-- 1 root   root    887 Jan 18 09:43 node1.csr
-rw-r--r-- 1 root   root   1675 Jan 18 09:44 node1-priv-key.pem
-rw-r--r-- 1 root   root   1082 Jan 18 09:49 node2-cert.pem
-rw-r--r-- 1 root   root    887 Jan 18 09:49 node2.csr
-rw-r--r-- 1 root   root   1675 Jan 18 09:49 node2-priv-key.pem
-rw-r--r-- 1 root   root   1082 Jan 18 09:42 swarm-cert.pem
-rw-r--r-- 1 root   root    887 Jan 18 09:41 swarm.csr
-rw-r--r-- 1 root   root   1679 Jan 18 09:42 swarm-priv-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can inspect the contents of each of the keys. To inspect a private key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl rsa -in &amp;lt;key-name&amp;gt; -noout -text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To inspect a public key (cert):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl x509 -in &amp;lt;key-name&amp;gt; -noout -text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following commands shows the partial contents of the Swarm Manager&amp;rsquo;s public
 &lt;code&gt;swarm-cert.pem&lt;/code&gt; key.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# openssl x509 -in ca.pem -noout -text
Certificate:
Data:
    Version: 3 (0x2)
    Serial Number: 9590646456311914051 (0x8518d2237ad49e43)
Signature Algorithm: sha256WithRSAEncryption
    Issuer: C=US, ST=CA, L=Sanfrancisco, O=Docker Inc
    Validity
        Not Before: Jan 18 09:42:16 2016 GMT
        Not After : Jan 15 09:42:16 2026 GMT
    Subject: CN=swarm

&amp;lt;output truncated&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-4-install-the-keys&#34;&gt;Step 4: Install the keys&lt;/h2&gt;

&lt;p&gt;In this step, you install the keys on the relevant servers in the
infrastructure. Each server needs three files:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A copy of the Certificate Authority&amp;rsquo;s public key (&lt;code&gt;ca.pem&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s own private key&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s own public key (cert)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The procedure below shows you how to copy these files from the CA server to each
server using &lt;code&gt;scp&lt;/code&gt;. As part of the copy procedure, you&amp;rsquo;ll rename each file as
follows on each node:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original name&lt;/th&gt;
&lt;th&gt;Copied name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ca.pem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ca.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;&amp;lt;server&amp;gt;-cert.pem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;cert.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;&amp;lt;server&amp;gt;-priv-key.pem&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;key.pem&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Logon to the terminal of your CA server and elevate to root.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo su
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a&lt;code&gt;~/.certs&lt;/code&gt; directory on the Swarm manager. Here we assume user account is ubuntu.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh ubuntu@swarm &#39;mkdir -p /home/ubuntu/.certs&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Copy the keys from the CA to the Swarm Manager server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scp ./ca.pem ubuntu@swarm:/home/ubuntu/.certs/ca.pem
$ scp ./swarm-cert.pem ubuntu@swarm:/home/ubuntu/.certs/cert.pem
$ scp ./swarm-priv-key.pem ubuntu@swarm:/home/ubuntu/.certs/key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You may need to provide authentication for the &lt;code&gt;scp&lt;/code&gt; commands to work. For example, AWS EC2 instances use certificate-based authentication. To copy the files to an EC2 instance associated with a public key called &lt;code&gt;nigel.pem&lt;/code&gt;, modify the &lt;code&gt;scp&lt;/code&gt; command as follows: &lt;code&gt;scp -i /path/to/nigel.pem ./ca.pem ubuntu@swarm:/home/ubuntu/.certs/ca.pem&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat step 2 for each remaining  server in the infrastructure.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;client&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify your work.&lt;/p&gt;

&lt;p&gt;When the copying is complete, each machine should have the following keys.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/tls-2.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each node in your infrastructure should have the following files in the
&lt;code&gt;/home/ubuntu/.certs/&lt;/code&gt; directory:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ls -l /home/ubuntu/.certs/
total 16
-rw-r--r-- 1 ubuntu ubuntu 1229 Jan 18 10:03 ca.pem
-rw-r--r-- 1 ubuntu ubuntu 1082 Jan 18 10:06 cert.pem
-rw-r--r-- 1 ubuntu ubuntu 1679 Jan 18 10:06 key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-5-configure-the-engine-daemon-for-tls&#34;&gt;Step 5: Configure the Engine daemon for TLS&lt;/h2&gt;

&lt;p&gt;In the last step, you created and installed the necessary keys on each of your
Swarm nodes. In this step, you configure them to listen on the network and only
accept connections using TLS. Once you complete this step, your Swarm nodes will
listen on TCP port 2376, and only accept connections using TLS.&lt;/p&gt;

&lt;p&gt;On &lt;code&gt;node1&lt;/code&gt; and &lt;code&gt;node2&lt;/code&gt; (your Swarm nodes), do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a terminal on &lt;code&gt;node1&lt;/code&gt; and elevate to root.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo su
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Edit Docker Engine configuration file.&lt;/p&gt;

&lt;p&gt;If you are following along with these instructions and using Ubuntu 14.04
LTS, the configuration file is &lt;code&gt;/etc/default/docker&lt;/code&gt;. The Docker Engine
configuration file may be different depending on the Linux distribution you
are using.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Add the following options to the &lt;code&gt;DOCKER_OPTS&lt;/code&gt; line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; -H tcp://0.0.0.0:2376 --tlsverify --tlscacert=/home/ubuntu/.certs/ca.pem --tlscert=/home/ubuntu/.certs/cert.pem --tlskey=/home/ubuntu/.certs/key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Restart the Docker Engine daemon.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ service docker restart
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat the procedure on &lt;code&gt;node2&lt;/code&gt; as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-6-create-a-swarm-cluster&#34;&gt;Step 6: Create a Swarm cluster&lt;/h2&gt;

&lt;p&gt;Next create a Swarm cluster. In this procedure you create a two-node Swarm
cluster using the default &lt;em&gt;hosted discovery&lt;/em&gt; backend. The default hosted
discovery backend uses Docker Hub and is not recommended for production use.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Logon to the terminal of your Swarm manager node.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create the cluster and export it&amp;rsquo;s unique ID to the &lt;code&gt;TOKEN&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo export TOKEN=$(docker run --rm swarm create)
Unable to find image &#39;swarm:latest&#39; locally
latest: Pulling from library/swarm
d681c900c6e3: Pulling fs layer
&amp;lt;snip&amp;gt;
986340ab62f0: Pull complete
a9975e2cc0a3: Pull complete
Digest: sha256:c21fd414b0488637b1f05f13a59b032a3f9da5d818d31da1a4ca98a84c0c781b
Status: Downloaded newer image for swarm:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Join &lt;code&gt;node1&lt;/code&gt; to the cluster.&lt;/p&gt;

&lt;p&gt;Be sure to specify TCP port &lt;code&gt;2376&lt;/code&gt; and not &lt;code&gt;2375&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d swarm join --addr=node1:2376 token://$TOKEN
7bacc98536ed6b4200825ff6f4004940eb2cec891e1df71c6bbf20157c5f9761
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Join &lt;code&gt;node2&lt;/code&gt; to the cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker run -d swarm join --addr=node2:2376 token://$TOKEN
db3f49d397bad957202e91f0679ff84f526e74d6c5bf1b6734d834f5edcbca6c
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-7-start-the-swarm-manager-using-tls&#34;&gt;Step 7: Start the Swarm Manager using TLS&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Launch a new container with TLS enables&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 3376:3376 -v /home/ubuntu/.certs:/certs:ro swarm manage --tlsverify --tlscacert=/certs/ca.pem --tlscert=/certs/cert.pem --tlskey=/certs/key.pem --host=0.0.0.0:3376 token://$TOKEN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command above launches a new container based on the &lt;code&gt;swarm&lt;/code&gt; image
and it maps port &lt;code&gt;3376&lt;/code&gt; on the server to port &lt;code&gt;3376&lt;/code&gt; inside the
container. This mapping ensures that Docker Engine commands sent to the host
on port &lt;code&gt;3376&lt;/code&gt; are passed on to port &lt;code&gt;3376&lt;/code&gt; inside the container. The
container runs the Swarm &lt;code&gt;manage&lt;/code&gt; process with the &lt;code&gt;--tlsverify&lt;/code&gt;,
&lt;code&gt;--tlscacert&lt;/code&gt;, &lt;code&gt;--tlscert&lt;/code&gt; and &lt;code&gt;--tlskey&lt;/code&gt; options specified. These options
force TLS verification and specify the location of the Swarm manager&amp;rsquo;s TLS
keys.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run a &lt;code&gt;docker ps&lt;/code&gt; command to verify that your Swarm manager container is up
and running.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
CONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                              NAMES
035dbf57b26e   swarm               &amp;quot;/swarm manage --tlsv&amp;quot;   7 seconds ago    Up 7 seconds    2375/tcp, 0.0.0.0:3376-&amp;gt;3376/tcp   compassionate_lovelace
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Your Swarm cluster is now configured to use TLS.&lt;/p&gt;

&lt;h2 id=&#34;step-8-test-the-swarm-manager-configuration&#34;&gt;Step 8: Test the Swarm manager configuration&lt;/h2&gt;

&lt;p&gt;Now that you have a Swarm cluster built and configured to use TLS, you&amp;rsquo;ll test that it works with a Docker Engine CLI.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a terminal onto your &lt;code&gt;client&lt;/code&gt; server.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Issue the &lt;code&gt;docker version&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;When issuing the command, you must pass it the location of the clients certifications.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker --tlsverify --tlscacert=/home/ubuntu/.certs/ca.pem --tlscert=/home/ubuntu/.certs/cert.pem --tlskey=/home/ubuntu/.certs/key.pem -H swarm:3376 version
Client:
 Version:      1.9.1
 API version:  1.21
 Go version:   go1.4.2
 Git commit:   a34a1d5
 Built:        Fri Nov 20 13:12:04 UTC 2015
 OS/Arch:      linux/amd64

Server:
 Version:      swarm/1.0.1
 API version:  1.21
 Go version:   go1.5.2
 Git commit:   744e3a3
 Built:
 OS/Arch:      linux/amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output above shows the &lt;code&gt;Server&lt;/code&gt; version as &amp;ldquo;swarm/1.0.1&amp;rdquo;. This means
that the command was successfully issued against the Swarm manager.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify that the same command does not work without TLS.&lt;/p&gt;

&lt;p&gt;This time, do not pass your certs to the Swarm manager.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo docker -H swarm:3376 version
:
 Version:      1.9.1
 API version:  1.21
 Go version:   go1.4.2
 Git commit:   a34a1d5
 Built:        Fri Nov 20 13:12:04 UTC 2015
 OS/Arch:      linux/amd64
Get http://swarm:3376/v1.21/version: malformed HTTP response &amp;quot;\x15\x03\x01\x00\x02\x02&amp;quot;.
* Are you trying to connect to a TLS-enabled daemon without TLS?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output above shows that the command was rejected by the server. This is
because the server (Swarm manager) is configured to only accept connections
from authenticated clients using TLS.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;step-9-configure-the-engine-cli-to-use-tls&#34;&gt;Step 9: Configure the Engine CLI to use TLS&lt;/h2&gt;

&lt;p&gt;You can configure the Engine so that you don&amp;rsquo;t have to pass the TLS options when
you issue a command. To do this, you&amp;rsquo;ll configure the &lt;code&gt;Docker Engine host&lt;/code&gt; and
&lt;code&gt;TLS&lt;/code&gt; settings as defaults on your Docker Engine client.&lt;/p&gt;

&lt;p&gt;To do this, you place the client&amp;rsquo;s keys in your &lt;code&gt;~/.docker&lt;/code&gt; configuration folder. If you have other users on your system using the Engine command line, you&amp;rsquo;ll need to configure their account&amp;rsquo;s &lt;code&gt;~/.docker&lt;/code&gt; as well. The procedure below shows how to do this for the &lt;code&gt;ubuntu&lt;/code&gt; user on
your Docker Engine client.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a terminal onto your &lt;code&gt;client&lt;/code&gt; server.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If it doesn&amp;rsquo;t exist, create a &lt;code&gt;.docker&lt;/code&gt; directory in the &lt;code&gt;ubuntu&lt;/code&gt; user&amp;rsquo;s home directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir /home/ubuntu/.docker
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Copy the Docker Engine client&amp;rsquo;s keys from &lt;code&gt;/home/ubuntu/.certs&lt;/code&gt; to
&lt;code&gt;/home/ubuntu/.docker&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp /home/ubuntu/.certs/{ca,cert,key}.pem /home/ubuntu/.docker
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Edit the account&amp;rsquo;s &lt;code&gt;~/.bash_profile&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Set the following variables:&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;DOCKER_HOST&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets the Docker host and TCP port to send all Engine commands to.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;DOCKER_TLS_VERIFY&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Tell&#39;s Engine to use TLS.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;DOCKER_CERT_PATH&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specifies the location of TLS keys.&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    export DOCKER_HOST=tcp://swarm:3376
    export DOCKER_TLS_VERIFY=1
    export DOCKER_CERT_PATH=/home/ubuntu/.docker/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Save and close the file.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Source the file to pick up the new variables.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    $ source ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify that the procedure worked by issuing a &lt;code&gt;docker version&lt;/code&gt; command&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker version
Client:
 Version:      1.9.1
 API version:  1.21
 Go version:   go1.4.2
 Git commit:   a34a1d5
 Built:        Fri Nov 20 13:12:04 UTC 2015
 OS/Arch:      linux/amd64

Server:
 Version:      swarm/1.0.1
 API version:  1.21
 Go version:   go1.5.2
 Git commit:   744e3a3
 Built:
 OS/Arch:      linux/amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The server portion of the output above command shows that your Docker
client is issuing commands to the Swarm Manager and using TLS.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Congratulations! You have configured a Docker Swarm cluster to use TLS.&lt;/p&gt;

&lt;h2 id=&#34;related-information&#34;&gt;Related Information&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/secure-swarm-tls/&#34;&gt;Secure Docker Swarm with TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/security/security/&#34;&gt;Docker security&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deploy application infrastructure</title>
      <link>http://docs-stage.docker.com/swarm/swarm_at_scale/deploy-infra/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/swarm_at_scale/deploy-infra/</guid>
      <description>

&lt;h1 id=&#34;deploy-your-infrastructure&#34;&gt;Deploy your infrastructure&lt;/h1&gt;

&lt;p&gt;In this step, you create several Docker hosts to run your application stack on.
Before you continue, make sure you have taken the time to &lt;a href=&#34;../swarm/swarm_at_scale/about/&#34;&gt;learn the application
architecture&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;about-these-instructions&#34;&gt;About these instructions&lt;/h2&gt;

&lt;p&gt;This example assumes you are running on a Mac or Windows system and enabling
Docker Engine &lt;code&gt;docker&lt;/code&gt; commands by provisioning local VirtualBox virtual
machines thru Docker Machine. For this evaluation installation, you&amp;rsquo;ll need 6 (six)
VirtualBox VMs.&lt;/p&gt;

&lt;p&gt;While this example uses Docker Machine, this is only one example of an
infrastructure you can use. You can create the environment design on whatever
infrastructure you wish. For example, you could place the application on another
public cloud platform such as Azure or DigitalOcean, on premises in your data
center, or even in in a test environment on your laptop.&lt;/p&gt;

&lt;p&gt;Finally, these instructions use some common &lt;code&gt;bash&lt;/code&gt; command substituion techniques to
resolve some values, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ eval $(docker-machine env keystore)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In a Windows environment, these substituation fail. If you are running in
Windows, replace the substitution &lt;code&gt;$(docker-machine env keystore)&lt;/code&gt; with the
actual value.&lt;/p&gt;

&lt;h2 id=&#34;task-1-create-the-keystore-server&#34;&gt;Task 1. Create the keystore server&lt;/h2&gt;

&lt;p&gt;To enable a Docker container network and Swarm discovery, you must
supply deploy a key-value store.  As a discovery backend, the keystore
maintains an up-to-date list of cluster members and shares that list with the
Swarm manager. The Swarm manager uses this list to assign tasks to the nodes.&lt;/p&gt;

&lt;p&gt;An overlay network requires a key-value store. The key-value store holds
information about the network state which includes discovery, networks,
endpoints, IP addresses, and more.&lt;/p&gt;

&lt;p&gt;Several different backends are supported. This example uses &lt;a
href=&#34;https://www.consul.io/&#34; target=&#34;blank&#34;&gt;Consul&lt;/a&gt; container.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a &amp;ldquo;machine&amp;rdquo; named &lt;code&gt;keystore&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-machine create -d virtualbox --virtualbox-memory &amp;quot;2000&amp;quot; \
--engine-opt=&amp;quot;label=com.function=consul&amp;quot;  keystore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can set options for the Engine daemon with the &lt;code&gt;--engine-opt&lt;/code&gt; flag. You&amp;rsquo;ll
use it to label this Engine instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Set your local shell to the &lt;code&gt;keystore&lt;/code&gt; Docker host.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ eval $(docker-machine env keystore)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run &lt;a href=&#34;https://hub.docker.com/r/progrium/consul/&#34; target=&#34;_blank&#34;&gt;the
&lt;code&gt;consul&lt;/code&gt; container&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run --restart=unless-stopped -d -p 8500:8500 -h consul progrium/consul -server -bootstrap
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-p&lt;/code&gt; flag publishes port 8500 on the container which is where the Consul
server listens. The server also has several other ports exposed which you can
see by running &lt;code&gt;docker ps&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker ps
CONTAINER ID        IMAGE               ...       PORTS                                                                            NAMES
372ffcbc96ed        progrium/consul     ...       53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500-&amp;gt;8500/tcp   dreamy_ptolemy
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use a &lt;code&gt;curl&lt;/code&gt; command test the server by listing the nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl $(docker-machine ip keystore):8500/v1/catalog/nodes
[{&amp;quot;Node&amp;quot;:&amp;quot;consul&amp;quot;,&amp;quot;Address&amp;quot;:&amp;quot;172.17.0.2&amp;quot;}]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;task-2-create-the-swarm-manager&#34;&gt;Task 2. Create the Swarm manager&lt;/h2&gt;

&lt;p&gt;In this step, you create the Swarm manager and connect it to the &lt;code&gt;keystore&lt;/code&gt;
instance. The Swarm manager container is the heart of your Swarm cluster.
It is responsible for receiving all Docker commands sent to the cluster, and for
scheduling resources against the cluster. In a real-world production deployment,
you should configure additional replica Swarm managers as secondaries for high
availability (HA).&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll use the &lt;code&gt;--eng-opt&lt;/code&gt; flag to set the &lt;code&gt;cluster-store&lt;/code&gt; and
&lt;code&gt;cluster-advertise&lt;/code&gt; options to refer to the &lt;code&gt;keystore&lt;/code&gt; server. These options
support the container network you&amp;rsquo;ll create later.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create the &lt;code&gt;manager&lt;/code&gt; host.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-machine create -d virtualbox --virtualbox-memory &amp;quot;2000&amp;quot; \
--engine-opt=&amp;quot;label=com.function=manager&amp;quot; \
--engine-opt=&amp;quot;cluster-store=consul://$(docker-machine ip keystore):8500&amp;quot; \
--engine-opt=&amp;quot;cluster-advertise=eth1:2376&amp;quot; manager
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You also give the daemon a &lt;code&gt;manager&lt;/code&gt; label.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Set your local shell to the &lt;code&gt;manager&lt;/code&gt; Docker host.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ eval $(docker-machine env manager)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the Swarm manager process.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run --restart=unless-stopped -d -p 3376:2375 \
-v /var/lib/boot2docker:/certs:ro \
swarm manage --tlsverify \
--tlscacert=/certs/ca.pem \
--tlscert=/certs/server.pem \
--tlskey=/certs/server-key.pem \
consul://$(docker-machine ip keystore):8500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command uses the TLS certificates created for the &lt;code&gt;boot2docker.iso&lt;/code&gt; or
the manager. This is key for the manager when it connects to other machines
in the cluster.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Test your work by using displaying the Docker daemon logs from the host.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-machine ssh manager
&amp;lt;-- output snipped --&amp;gt;
docker@manager:~$ tail /var/lib/boot2docker/docker.log
time=&amp;quot;2016-04-06T23:11:56.481947896Z&amp;quot; level=debug msg=&amp;quot;Calling GET /v1.15/version&amp;quot;
time=&amp;quot;2016-04-06T23:11:56.481984742Z&amp;quot; level=debug msg=&amp;quot;GET /v1.15/version&amp;quot;
time=&amp;quot;2016-04-06T23:12:13.070231761Z&amp;quot; level=debug msg=&amp;quot;Watch triggered with 1 nodes&amp;quot; discovery=consul
time=&amp;quot;2016-04-06T23:12:33.069387215Z&amp;quot; level=debug msg=&amp;quot;Watch triggered with 1 nodes&amp;quot; discovery=consul
time=&amp;quot;2016-04-06T23:12:53.069471308Z&amp;quot; level=debug msg=&amp;quot;Watch triggered with 1 nodes&amp;quot; discovery=consul
time=&amp;quot;2016-04-06T23:13:13.069512320Z&amp;quot; level=debug msg=&amp;quot;Watch triggered with 1 nodes&amp;quot; discovery=consul
time=&amp;quot;2016-04-06T23:13:33.070021418Z&amp;quot; level=debug msg=&amp;quot;Watch triggered with 1 nodes&amp;quot; discovery=consul
time=&amp;quot;2016-04-06T23:13:53.069395005Z&amp;quot; level=debug msg=&amp;quot;Watch triggered with 1 nodes&amp;quot; discovery=consul
time=&amp;quot;2016-04-06T23:14:13.071417551Z&amp;quot; level=debug msg=&amp;quot;Watch triggered with 1 nodes&amp;quot; discovery=consul
time=&amp;quot;2016-04-06T23:14:33.069843647Z&amp;quot; level=debug msg=&amp;quot;Watch triggered with 1 nodes&amp;quot; discovery=consul
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output indicates that the &lt;code&gt;consul&lt;/code&gt; and the &lt;code&gt;manager&lt;/code&gt; are communicating correctly.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Exit the Docker host.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker@manager:~$ exit
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;task-3-add-the-load-balancer&#34;&gt;Task 3. Add the load balancer&lt;/h2&gt;

&lt;p&gt;The application uses &lt;a
href=&#34;https://github.com/ehazlett/interlock&#34;&gt;Interlock&lt;/a&gt; and Nginx as a
loadbalancer. Before you build the load balancer host, you&amp;rsquo;ll create the
configuration you&amp;rsquo;ll use for Nginx.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;On your local host, create a &lt;code&gt;config&lt;/code&gt; directory.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Change directories to the &lt;code&gt;config&lt;/code&gt; directory.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd config
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get the IP address of the Swarm manager host.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-machine ip manager
192.168.99.101
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use your favorite editor to create a &lt;code&gt;config.toml&lt;/code&gt; file and add this content
to the file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;ListenAddr = &amp;quot;:8080&amp;quot;
DockerURL = &amp;quot;tcp://SWARM_MANAGER_IP:3376&amp;quot;
TLSCACert = &amp;quot;/var/lib/boot2docker/ca.pem&amp;quot;
TLSCert = &amp;quot;/var/lib/boot2docker/server.pem&amp;quot;
TLSKey = &amp;quot;/var/lib/boot2docker/server-key.pem&amp;quot;

[[Extensions]]
Name = &amp;quot;nginx&amp;quot;
ConfigPath = &amp;quot;/etc/conf/nginx.conf&amp;quot;
PidPath = &amp;quot;/etc/conf/nginx.pid&amp;quot;
MaxConn = 1024
Port = 80
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the configuration, replace the &lt;code&gt;SWARM_MANAGER_IP&lt;/code&gt; with the &lt;code&gt;manager&lt;/code&gt; IP you got
in Step 4.&lt;/p&gt;

&lt;p&gt;You use this value because the load balancer listens on the manager&amp;rsquo;s event
stream.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Save and close the &lt;code&gt;config.toml&lt;/code&gt; file.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a machine for the load balancer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-machine create -d virtualbox --virtualbox-memory &amp;quot;2000&amp;quot; \
--engine-opt=&amp;quot;label=com.function=interlock&amp;quot; loadbalancer
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Switch the environment to the &lt;code&gt;loadbalancer&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ eval $(docker-machine env loadbalancer)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start an &lt;code&gt;interlock&lt;/code&gt; container running.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run \
    -P \
    -d \
    -ti \
    -v nginx:/etc/conf \
    -v /var/lib/boot2docker:/var/lib/boot2docker:ro \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v $(pwd)/config.toml:/etc/config.toml \
    --name interlock \
    ehazlett/interlock:1.0.1 \
    -D run -c /etc/config.toml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command relies on the &lt;code&gt;config.toml&lt;/code&gt; file being in the current directory.  After running the command, confirm the image is runing:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker ps
CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS                     NAMES
d846b801a978        ehazlett/interlock:1.0.1   &amp;quot;/bin/interlock -D ru&amp;quot;   2 minutes ago       Up 2 minutes        0.0.0.0:32770-&amp;gt;8080/tcp   interlock
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you don&amp;rsquo;t see the image runing, use &lt;code&gt;docker ps -a&lt;/code&gt; to list all images to make sure the system attempted to start the image. Then, get the logs to see why the container failed to start.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker logs interlock
INFO[0000] interlock 1.0.1 (000291d)
DEBU[0000] loading config from: /etc/config.toml
FATA[0000] read /etc/config.toml: is a directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This error usually means you weren&amp;rsquo;t starting the &lt;code&gt;docker run&lt;/code&gt; from the same
&lt;code&gt;config&lt;/code&gt; directory where the &lt;code&gt;config.toml&lt;/code&gt; file is. If you run the command
and get a Conflict error such as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker: Error response from daemon: Conflict. The name &amp;quot;/interlock&amp;quot; is already in use by container d846b801a978c76979d46a839bb05c26d2ab949ff9f4f740b06b5e2564bae958. You have to remove (or rename) that container to be able to reuse that name.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove the interlock container with the &lt;code&gt;docker rm interlock&lt;/code&gt; and try again.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start an &lt;code&gt;nginx&lt;/code&gt; container on the load balancer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -ti -d \
  -p 80:80 \
  --label interlock.ext.name=nginx \
  --link=interlock:interlock \
  -v nginx:/etc/conf \
  --name nginx \
  nginx nginx -g &amp;quot;daemon off;&amp;quot; -c /etc/conf/nginx.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;task-4-create-the-other-swarm-nodes&#34;&gt;Task 4. Create the other Swarm nodes&lt;/h2&gt;

&lt;p&gt;A host in a Swarm cluster is called a &lt;em&gt;node&lt;/em&gt;. You&amp;rsquo;ve already created the manager
node. Here, the task is to create each virtual host for each node. There are
three commands required:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;create the host with Docker Machine&lt;/li&gt;
&lt;li&gt;point the local environment to the new host&lt;/li&gt;
&lt;li&gt;join the host to the Swarm cluster&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you were building this in a non-Mac/Windows environment, you&amp;rsquo;d only need to
run the &lt;code&gt;join&lt;/code&gt; command to add a node to the Swarm cluster and register it with
the Consul discovery service. When you create a node, you also give it a label,
for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--engine-opt=&amp;quot;label=com.function=frontend01&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll use these labels later when starting application containers. In the
commands below, notice the label you are applying to each node.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create the &lt;code&gt;frontend01&lt;/code&gt; host and add it to the Swarm cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-machine create -d virtualbox --virtualbox-memory &amp;quot;2000&amp;quot; \
--engine-opt=&amp;quot;label=com.function=frontend01&amp;quot; \
--engine-opt=&amp;quot;cluster-store=consul://$(docker-machine ip keystore):8500&amp;quot; \
--engine-opt=&amp;quot;cluster-advertise=eth1:2376&amp;quot; frontend01
$ eval $(docker-machine env frontend01)
$ docker run -d swarm join --addr=$(docker-machine ip frontend01):2376 consul://$(docker-machine ip keystore):8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create the &lt;code&gt;frontend02&lt;/code&gt; VM.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-machine create -d virtualbox --virtualbox-memory &amp;quot;2000&amp;quot; \
--engine-opt=&amp;quot;label=com.function=frontend02&amp;quot; \
--engine-opt=&amp;quot;cluster-store=consul://$(docker-machine ip keystore):8500&amp;quot; \
--engine-opt=&amp;quot;cluster-advertise=eth1:2376&amp;quot; frontend02
$ eval $(docker-machine env frontend02)
$ docker run -d swarm join --addr=$(docker-machine ip frontend02):2376 consul://$(docker-machine ip keystore):8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create the &lt;code&gt;worker01&lt;/code&gt; VM.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-machine create -d virtualbox --virtualbox-memory &amp;quot;2000&amp;quot; \
--engine-opt=&amp;quot;label=com.function=worker01&amp;quot; \
--engine-opt=&amp;quot;cluster-store=consul://$(docker-machine ip keystore):8500&amp;quot; \
--engine-opt=&amp;quot;cluster-advertise=eth1:2376&amp;quot; worker01
$ eval $(docker-machine env worker01)
$ docker run -d swarm join --addr=$(docker-machine ip worker01):2376 consul://$(docker-machine ip keystore):8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create the &lt;code&gt;dbstore&lt;/code&gt; VM.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-machine create -d virtualbox --virtualbox-memory &amp;quot;2000&amp;quot; \
--engine-opt=&amp;quot;label=com.function=dbstore&amp;quot; \
--engine-opt=&amp;quot;cluster-store=consul://$(docker-machine ip keystore):8500&amp;quot; \
--engine-opt=&amp;quot;cluster-advertise=eth1:2376&amp;quot; dbstore
$ eval $(docker-machine env dbstore)
$ docker run -d swarm join --addr=$(docker-machine ip dbstore):2376 consul://$(docker-machine ip keystore):8500
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check your work.&lt;/p&gt;

&lt;p&gt;At this point, you have deployed on the infrastructure you need to run the
application. Test this now by listing the running machines:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-machine ls
NAME           ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER    ERRORS
dbstore        -        virtualbox   Running   tcp://192.168.99.111:2376           v1.10.3
frontend01     -        virtualbox   Running   tcp://192.168.99.108:2376           v1.10.3
frontend02     -        virtualbox   Running   tcp://192.168.99.109:2376           v1.10.3
keystore       -        virtualbox   Running   tcp://192.168.99.100:2376           v1.10.3
loadbalancer   -        virtualbox   Running   tcp://192.168.99.107:2376           v1.10.3
manager        -        virtualbox   Running   tcp://192.168.99.101:2376           v1.10.3
worker01       *        virtualbox   Running   tcp://192.168.99.110:2376           v1.10.3
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Make sure the Swarm manager sees all your nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker -H $(docker-machine ip manager):3376 info
Containers: 4
 Running: 4
 Paused: 0
 Stopped: 0
Images: 3
Server Version: swarm/1.1.3
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 4
 dbstore: 192.168.99.111:2376
   Status: Healthy
   Containers: 1
   Reserved CPUs: 0 / 1
   Reserved Memory: 0 B / 2.004 GiB
   Labels: com.function=dbstore, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs
   Error: (none)
   UpdatedAt: 2016-04-07T18:25:37Z
 frontend01: 192.168.99.108:2376
   Status: Healthy
   Containers: 1
   Reserved CPUs: 0 / 1
   Reserved Memory: 0 B / 2.004 GiB
   Labels: com.function=frontend01, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs
   Error: (none)
   UpdatedAt: 2016-04-07T18:26:10Z
 frontend02: 192.168.99.109:2376
   Status: Healthy
   Containers: 1
   Reserved CPUs: 0 / 1
   Reserved Memory: 0 B / 2.004 GiB
   Labels: com.function=frontend02, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs
   Error: (none)
   UpdatedAt: 2016-04-07T18:25:43Z
 worker01: 192.168.99.110:2376
   Status: Healthy
   Containers: 1
   Reserved CPUs: 0 / 1
   Reserved Memory: 0 B / 2.004 GiB
   Labels: com.function=worker01, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs
   Error: (none)
   UpdatedAt: 2016-04-07T18:25:56Z
Plugins:
 Volume:
 Network:
Kernel Version: 4.1.19-boot2docker
Operating System: linux
Architecture: amd64
CPUs: 4
Total Memory: 8.017 GiB
Name: bb13b7cf80e8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command is acting on the Swarm port, so it returns information about the
entire cluster. You have a manager and no nodes.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;next-step&#34;&gt;Next Step&lt;/h2&gt;

&lt;p&gt;Your key-store, load balancer, and Swarm cluster infrastructure is up. You are
ready to &lt;a href=&#34;../swarm/swarm_at_scale/deploy-app/&#34;&gt;build and run the voting application&lt;/a&gt; on it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploy the application</title>
      <link>http://docs-stage.docker.com/swarm/swarm_at_scale/deploy-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/swarm_at_scale/deploy-app/</guid>
      <description>

&lt;h1 id=&#34;deploy-the-application&#34;&gt;Deploy the application&lt;/h1&gt;

&lt;p&gt;You&amp;rsquo;ve &lt;a href=&#34;../swarm/swarm_at_scale/deploy-infra/&#34;&gt;deployed the load balancer, the discovery backend, and a Swarm
cluster&lt;/a&gt; so now you can build and deploy the voting application
itself. You do this by starting a number of &amp;ldquo;Dockerized applications&amp;rdquo; running in
containers.&lt;/p&gt;

&lt;p&gt;The diagram below shows the final application configuration including the overlay
container network, &lt;code&gt;voteapp&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/final-result.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this procedure you will connect containers to this network. The &lt;code&gt;voteapp&lt;/code&gt;
network is available to all Docker hosts using the Consul discovery backend.
Notice that the &lt;code&gt;interlock&lt;/code&gt;, &lt;code&gt;nginx&lt;/code&gt;, &lt;code&gt;consul&lt;/code&gt;, and &lt;code&gt;swarm manager&lt;/code&gt; containers
on are not part of the &lt;code&gt;voteapp&lt;/code&gt; overlay container network.&lt;/p&gt;

&lt;h2 id=&#34;task-1-set-up-volume-and-network&#34;&gt;Task 1. Set up volume and network&lt;/h2&gt;

&lt;p&gt;This application relies on both an overlay container network and a container
volume. The Docker Engine provides these two features. You&amp;rsquo;ll create them both
on the Swarm &lt;code&gt;manager&lt;/code&gt; instance.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Direct your local environment to the Swarm manager host.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ eval $(docker-machine env manager)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can create the network on a cluster node at the network is visible on
them all.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create the &lt;code&gt;voteapp&lt;/code&gt; container network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker network create -d overlay voteapp
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Switch to the db store.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ eval $(docker-machine env dbstore)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Verify you can see the new network from the dbstore node.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker network ls
NETWORK ID          NAME                DRIVER
e952814f610a        voteapp             overlay
1f12c5e7bcc4        bridge              bridge
3ca38e887cd8        none                null
3da57c44586b        host                host
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a container volume called &lt;code&gt;db-data&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker volume create --name db-data
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;task-2-start-the-containerized-microservices&#34;&gt;Task 2. Start the containerized microservices&lt;/h2&gt;

&lt;p&gt;At this point, you are ready to start the component microservices that make up
the application. Some of the application&amp;rsquo;s containers are launched from existing
images pulled directly from Docker Hub. Other containers are launched from
custom images you must build. The list below shows which containers use custom
images and which do not:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Load balancer container: stock image (&lt;code&gt;ehazlett/interlock&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Redis containers: stock image (official &lt;code&gt;redis&lt;/code&gt; image)&lt;/li&gt;
&lt;li&gt;Postgres (PostgreSQL) containers: stock image (official &lt;code&gt;postgres&lt;/code&gt; image)&lt;/li&gt;
&lt;li&gt;Web containers: custom built image&lt;/li&gt;
&lt;li&gt;Worker containers: custom built image&lt;/li&gt;
&lt;li&gt;Results containers: custom built image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can launch these containers from any host in the cluster using the commands
in this section. Each command includs a &lt;code&gt;-H&lt;/code&gt;flag so that they execute against
the Swarm manager.&lt;/p&gt;

&lt;p&gt;The commands also all use the &lt;code&gt;-e&lt;/code&gt; flag which is a Swarm constraint. The
constraint tells the manager to look for a node with a matching function label.
You set established the labels when you created the nodes. As you run each
command below, look for the value constraint.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Start a Postgres database container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H $(docker-machine ip manager):3376 run -t -d \
-v db-data:/var/lib/postgresql/data \
-e constraint:com.function==dbstore \
--net=&amp;quot;voteapp&amp;quot; \
--name db postgres:9.4
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the Redis container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H $(docker-machine ip manager):3376 run -t -d \
-p 6379:6379 \
-e constraint:com.function==dbstore \
--net=&amp;quot;voteapp&amp;quot; \
--name redis redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;redis&lt;/code&gt; name is important so don&amp;rsquo;t change it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the worker application&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H $(docker-machine ip manager):3376 run -t -d \
-e constraint:com.function==worker01 \
--net=&amp;quot;voteapp&amp;quot; \
--net-alias=workers \
--name worker01 docker/example-voting-app-worker
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the results application.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H $(docker-machine ip manager):3376 run -t -d \
-p 80:80 \
--label=interlock.hostname=results \
--label=interlock.domain=myenterprise.com \
-e constraint:com.function==dbstore \
--net=&amp;quot;voteapp&amp;quot; \
--name results-app docker/example-voting-app-result-app
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the voting application twice; once on each frontend node.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H $(docker-machine ip manager):3376 run -t -d \
-p 80:80 \
--label=interlock.hostname=vote \
--label=interlock.domain=myenterprise.com \
-e constraint:com.function==frontend01 \
--net=&amp;quot;voteapp&amp;quot; \
--name voting-app01 docker/example-voting-app-voting-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And again on the other frontend node.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H $(docker-machine ip manager):3376 run -t -d \
-p 80:80 \
--label=interlock.hostname=vote \
--label=interlock.domain=myenterprise.com \
-e constraint:com.function==frontend02 \
--net=&amp;quot;voteapp&amp;quot; \
--name voting-app02 docker/example-voting-app-voting-app
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;task-3-check-your-work-and-update-etc-hosts&#34;&gt;Task 3. Check your work and update /etc/hosts&lt;/h2&gt;

&lt;p&gt;In this step, you check your work to make sure the Nginx configuration recorded
the containers correctly. You&amp;rsquo;ll update your local systems &lt;code&gt;/etc/hosts&lt;/code&gt; file to
allow you to take advantage of the loadbalancer.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Change to the &lt;code&gt;loadbalancer&lt;/code&gt; node.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ eval $(docker-machine env loadbalancer)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check your work by reviewing the configuration of nginx.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;$ docker exec interlock cat /etc/conf/nginx.conf
... output snipped ...

upstream results.myenterprise.com {
    zone results.myenterprise.com_backend 64k;

    server 192.168.99.111:80;

}
server {
    listen 80;

    server_name results.myenterprise.com;

    location / {
        proxy_pass http://results.myenterprise.com;
    }
}
upstream vote.myenterprise.com {
    zone vote.myenterprise.com_backend 64k;

    server 192.168.99.109:80;
    server 192.168.99.108:80;

}
server {
    listen 80;

    server_name vote.myenterprise.com;

    location / {
        proxy_pass http://vote.myenterprise.com;
    }
}

include /etc/conf/conf.d/*.conf;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;code&gt;http://vote.myenterprise.com&lt;/code&gt; site configuration should point to either
  frontend node. Requests to &lt;code&gt;http://results.myenterprise.com&lt;/code&gt; go just to the
  single &lt;code&gt;dbstore&lt;/code&gt; node where the &lt;code&gt;example-voting-app-result-app&lt;/code&gt; is running.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;On your local host, edit &lt;code&gt;/etc/hosts&lt;/code&gt; file add the resolution for both these
sites.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Save and close the &lt;code&gt;/etc/hosts&lt;/code&gt; file.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Restart the &lt;code&gt;nginx&lt;/code&gt; container.&lt;/p&gt;

&lt;p&gt;Manual restart is required because the current Interlock server is not forcing an
Nginx configuration reload.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker restart nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;task-4-test-the-application&#34;&gt;Task 4. Test the application&lt;/h2&gt;

&lt;p&gt;Now, you can test your application.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a browser and navigate to the &lt;code&gt;http://vote.myenterprise.com&lt;/code&gt; site.&lt;/p&gt;

&lt;p&gt;You should see something similar to the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/vote-app-test.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click on one of the two voting options.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Navigate to the &lt;code&gt;http://results.myenterprise.com&lt;/code&gt; site to see the results.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Try changing your vote.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll see both sides change as you switch your vote.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/votes.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;extra-credit-deployment-with-docker-compose&#34;&gt;Extra Credit: Deployment with Docker Compose&lt;/h2&gt;

&lt;p&gt;Up to this point, you&amp;rsquo;ve deployed each application container individually. This
can be cumbersome espeically because their are several different containers and
starting them is order dependent. For example, that database should be running
before the worker.&lt;/p&gt;

&lt;p&gt;Docker Compose let&amp;rsquo;s you define your microservice containers and their
dependencies in a Compose file. Then, you can use the Compose file to start all
the containers at once. This extra credit&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Before you begin, stop all the containers you started.&lt;/p&gt;

&lt;p&gt;a. Set the host to the manager.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ DOCKER_HOST=$(docker-machine ip manager):3376
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;b. List all the application continers on the Swarm.&lt;/p&gt;

&lt;p&gt;c. Stop and remove each container.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Try to create Compose file on your own by reviewing the tasks in this tutorial.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../compose/compose-file/&#34;&gt;The version 2 Compose file format&lt;/a&gt;
is the best to use. Translate each &lt;code&gt;docker run&lt;/code&gt; command into a service
in the &lt;code&gt;docker-compose.yml&lt;/code&gt; file. For example,
this command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H $(docker-machine ip manager):3376 run -t -d \
-e constraint:com.function==worker01 \
--net=&amp;quot;voteapp&amp;quot; \
--net-alias=workers \
--name worker01 docker/example-voting-app-worker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Becomes this in a Compose file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;worker:
  image: docker/example-voting-app-worker
  networks:
    voteapp:
      aliases:
      - workers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In general, Compose starts services in reverse order they appear in the file.
So, if you want a service to start before all the others, make it the last
service in the file file.  This applciation relies on a volume and a network,
declare those at the bottom of the file.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check your work against &lt;a href=&#34;../docker-compose.yml&#34; target=&#34;_blank&#34;&gt;this
result file&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When you are satisifed, save the &lt;code&gt;docker-compose.yml&lt;/code&gt; file to your system.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Set  &lt;code&gt;DOCKER_HOST&lt;/code&gt; to the Swarm manager.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ DOCKER_HOST=$(docker-machine ip manager):3376
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the same directory as your &lt;code&gt;docker-compose.yml&lt;/code&gt; file, start the services.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose up -d
Creating network &amp;quot;scale_voteapp&amp;quot; with the default driver
Creating volume &amp;quot;scale_db-data&amp;quot; with default driver
Pulling db (postgres:9.4)...
worker01: Pulling postgres:9.4... : downloaded
dbstore: Pulling postgres:9.4... : downloaded
frontend01: Pulling postgres:9.4... : downloaded
frontend02: Pulling postgres:9.4... : downloaded
Creating db
Pulling redis (redis:latest)...
dbstore: Pulling redis:latest... : downloaded
frontend01: Pulling redis:latest... : downloaded
frontend02: Pulling redis:latest... : downloaded
worker01: Pulling redis:latest... : downloaded
Creating redis
Pulling worker (docker/example-voting-app-worker:latest)...
dbstore: Pulling docker/example-voting-app-worker:latest... : downloaded
frontend01: Pulling docker/example-voting-app-worker:latest... : downloaded
frontend02: Pulling docker/example-voting-app-worker:latest... : downloaded
worker01: Pulling docker/example-voting-app-worker:latest... : downloaded
Creating scale_worker_1
Pulling voting-app (docker/example-voting-app-voting-app:latest)...
dbstore: Pulling docker/example-voting-app-voting-app:latest... : downloaded
frontend01: Pulling docker/example-voting-app-voting-app:latest... : downloaded
frontend02: Pulling docker/example-voting-app-voting-app:latest... : downloaded
worker01: Pulling docker/example-voting-app-voting-app:latest... : downloaded
Creating scale_voting-app_1
Pulling result-app (docker/example-voting-app-result-app:latest)...
dbstore: Pulling docker/example-voting-app-result-app:latest... : downloaded
frontend01: Pulling docker/example-voting-app-result-app:latest... : downloaded
frontend02: Pulling docker/example-voting-app-result-app:latest... : downloaded
worker01: Pulling docker/example-voting-app-result-app:latest... : downloaded
Creating scale_result-app_1
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;docker ps&lt;/code&gt; command to see the containers on the Swarm cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H $(docker-machine ip manager):3376 ps
CONTAINER ID        IMAGE                                  COMMAND                  CREATED             STATUS              PORTS                            NAMES
b71555033caa        docker/example-voting-app-result-app   &amp;quot;node server.js&amp;quot;         6 seconds ago       Up 4 seconds        192.168.99.104:32774-&amp;gt;80/tcp     frontend01/scale_result-app_1
cf29ea21475d        docker/example-voting-app-worker       &amp;quot;/usr/lib/jvm/java-7-&amp;quot;   6 seconds ago       Up 4 seconds                                         worker01/scale_worker_1
98414cd40ab9        redis                                  &amp;quot;/entrypoint.sh redis&amp;quot;   7 seconds ago       Up 5 seconds        192.168.99.105:32774-&amp;gt;6379/tcp   frontend02/redis
1f214acb77ae        postgres:9.4                           &amp;quot;/docker-entrypoint.s&amp;quot;   7 seconds ago       Up 5 seconds        5432/tcp                         frontend01/db
1a4b8f7ce4a9        docker/example-voting-app-voting-app   &amp;quot;python app.py&amp;quot;          7 seconds ago       Up 5 seconds        192.168.99.107:32772-&amp;gt;80/tcp     dbstore/scale_voting-app_1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you started the services manually, you had a
&lt;code&gt;voting-app&lt;/code&gt; instances running on two frontend servers. How many
do you have now?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Scale your application up by adding some &lt;code&gt;voting-app&lt;/code&gt; instances.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose scale voting-app=3
Creating and starting 2 ... done
Creating and starting 3 ... done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After you scale up, list the containers on the cluster again.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Change to the &lt;code&gt;loadbalancer&lt;/code&gt; node.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ eval $(docker-machine env loadbalancer)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Restart the Nginx server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker restart nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check your work again by visiting the &lt;code&gt;http://vote.myenterprise.com&lt;/code&gt; and
&lt;code&gt;http://results.myenterprise.com&lt;/code&gt; again.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You can view the logs on an indvidual container.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  $ docker logs scale_voting-app_1
   * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
   * Restarting with stat
   * Debugger is active!
   * Debugger pin code: 285-809-660
  192.168.99.103 - - [11/Apr/2016 17:15:44] &amp;quot;GET / HTTP/1.0&amp;quot; 200 -
  192.168.99.103 - - [11/Apr/2016 17:15:44] &amp;quot;GET /static/stylesheets/style.css HTTP/1.0&amp;quot; 304 -
  192.168.99.103 - - [11/Apr/2016 17:15:45] &amp;quot;GET /favicon.ico HTTP/1.0&amp;quot; 404 -
  192.168.99.103 - - [11/Apr/2016 17:22:24] &amp;quot;POST / HTTP/1.0&amp;quot; 200 -
  192.168.99.103 - - [11/Apr/2016 17:23:37] &amp;quot;POST / HTTP/1.0&amp;quot; 200 -
  192.168.99.103 - - [11/Apr/2016 17:23:39] &amp;quot;POST / HTTP/1.0&amp;quot; 200 -
  192.168.99.103 - - [11/Apr/2016 17:23:40] &amp;quot;POST / HTTP/1.0&amp;quot; 200 -
  192.168.99.103 - - [11/Apr/2016 17:23:41] &amp;quot;POST / HTTP/1.0&amp;quot; 200 -
  192.168.99.103 - - [11/Apr/2016 17:23:43] &amp;quot;POST / HTTP/1.0&amp;quot; 200 -
  192.168.99.103 - - [11/Apr/2016 17:23:44] &amp;quot;POST / HTTP/1.0&amp;quot; 200 -
  192.168.99.103 - - [11/Apr/2016 17:23:46] &amp;quot;POST / HTTP/1.0&amp;quot; 200 -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This log shows the activity on one of the active voting application containers.&lt;/p&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;Congratulations. You have successfully walked through manually deploying a
microservice-based application to a Swarm cluster. Of course, not every
deployment goes smoothly. Now that you&amp;rsquo;ve learned how to successfully deploy an
application at scale, you should learn &lt;a href=&#34;../swarm/swarm_at_scale/troubleshoot/&#34;&gt;what to consider when troubleshooting
large applications running on a Swarm cluster&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Discovery</title>
      <link>http://docs-stage.docker.com/swarm/discovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/discovery/</guid>
      <description>

&lt;h1 id=&#34;docker-swarm-discovery&#34;&gt;Docker Swarm Discovery&lt;/h1&gt;

&lt;p&gt;Docker Swarm comes with multiple discovery backends. You use a hosted discovery service with Docker Swarm. The service maintains a list of IPs in your cluster.
This page describes the different types of hosted discovery available to you. These are:&lt;/p&gt;

&lt;h2 id=&#34;using-a-distributed-key-value-store&#34;&gt;Using a distributed key/value store&lt;/h2&gt;

&lt;p&gt;The recommended way to do node discovery in Swarm is Docker&amp;rsquo;s libkv project. The libkv project is an abstraction layer over existing distributed key/value stores.  As of this writing, the project supports:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Consul 0.5.1 or higher&lt;/li&gt;
&lt;li&gt;Etcd 2.0 or higher&lt;/li&gt;
&lt;li&gt;ZooKeeper 3.4.5 or higher&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For details about libkv and a detailed technical overview of the supported backends, refer to the &lt;a href=&#34;https://github.com/docker/libkv&#34;&gt;libkv project&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;using-a-hosted-discovery-key-store&#34;&gt;Using a hosted discovery key store&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;On each node, start the Swarm agent.&lt;/p&gt;

&lt;p&gt;The node IP address doesn&amp;rsquo;t have to be public as long as the Swarm manager can access it. In a large cluster, the nodes joining swarm may trigger request spikes to discovery. For example, a large number of nodes are added by a script, or recovered from a network partition. This may result in discovery failure. You can use &lt;code&gt;--delay&lt;/code&gt; option to specify a delay limit. Swarm join will add a random delay less than this limit to reduce pressure to discovery.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Etcd&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; etcd://&amp;lt;etcd_addr1&amp;gt;,&amp;lt;etcd_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Consul&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ZooKeeper&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; zk://&amp;lt;zookeeper_addr1&amp;gt;,&amp;lt;zookeeper_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the Swarm manager on any machine or your laptop.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Etcd&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; etcd://&amp;lt;etcd_addr1&amp;gt;,&amp;lt;etcd_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Consul&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ZooKeeper&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; zk://&amp;lt;zookeeper_addr1&amp;gt;,&amp;lt;zookeeper_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Try listing the nodes in your cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Etcd&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list etcd://&amp;lt;etcd_addr1&amp;gt;,&amp;lt;etcd_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Consul&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ZooKeeper&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list zk://&amp;lt;zookeeper_addr1&amp;gt;,&amp;lt;zookeeper_addr2&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;use-tls-with-distributed-key-value-discovery&#34;&gt;Use TLS with distributed key/value discovery&lt;/h3&gt;

&lt;p&gt;You can securely talk to the distributed k/v store using TLS. To connect
securely to the store, you must generate the certificates for a node when you
&lt;code&gt;join&lt;/code&gt; it to the swarm. You can only use with Consul and Etcd. The following example illustrates this with Consul:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm join \
    --advertise=&amp;lt;node_ip:2375&amp;gt; \
    --discovery-opt kv.cacertfile=/path/to/mycacert.pem \
    --discovery-opt kv.certfile=/path/to/mycert.pem \
    --discovery-opt kv.keyfile=/path/to/mykey.pem \
    consul://&amp;lt;consul_addr&amp;gt;/&amp;lt;optional path prefix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works the same way for the Swarm &lt;code&gt;manage&lt;/code&gt; and &lt;code&gt;list&lt;/code&gt; commands.&lt;/p&gt;

&lt;h2 id=&#34;a-static-file-or-list-of-nodes&#34;&gt;A static file or list of nodes&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;*: This discovery method is incompatible with replicating Swarm
managers. If you require replication, you should use a hosted discovery key
store.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can use a static file or list of nodes for your discovery backend. The file must be stored on a host that is accessible from the Swarm manager. You can also pass a node list as an option when you start Swarm.&lt;/p&gt;

&lt;p&gt;Both the static file and the &lt;code&gt;nodes&lt;/code&gt; option support an IP address ranges. To specify a range supply a pattern, for example, &lt;code&gt;10.0.0.[10:200]&lt;/code&gt; refers to nodes starting from &lt;code&gt;10.0.0.10&lt;/code&gt; to &lt;code&gt;10.0.0.200&lt;/code&gt;.  For example for the &lt;code&gt;file&lt;/code&gt; discovery method.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    $ echo &amp;quot;10.0.0.[11:100]:2375&amp;quot;   &amp;gt;&amp;gt; /tmp/my_cluster
    $ echo &amp;quot;10.0.1.[15:20]:2375&amp;quot;    &amp;gt;&amp;gt; /tmp/my_cluster
    $ echo &amp;quot;192.168.1.2:[2:20]375&amp;quot;  &amp;gt;&amp;gt; /tmp/my_cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or with node discovery:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    swarm manage -H &amp;lt;swarm_ip:swarm_port&amp;gt; &amp;quot;nodes://10.0.0.[10:200]:2375,10.0.1.[2:250]:2375&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;to-create-a-file&#34;&gt;To create a file&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Edit the file and add line for each of your nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;lt;node_ip1:2375&amp;gt; &amp;gt;&amp;gt; /opt/my_cluster
echo &amp;lt;node_ip2:2375&amp;gt; &amp;gt;&amp;gt; /opt/my_cluster
echo &amp;lt;node_ip3:2375&amp;gt; &amp;gt;&amp;gt; /opt/my_cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example creates a file named &lt;code&gt;/tmp/my_cluster&lt;/code&gt;. You can use any name you like.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the Swarm manager on any machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; file:///tmp/my_cluster
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;List the nodes in your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm list file:///tmp/my_cluster
&amp;lt;node_ip1:2375&amp;gt;
&amp;lt;node_ip2:2375&amp;gt;
&amp;lt;node_ip3:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;to-use-a-node-list&#34;&gt;To use a node list&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Start the manager on any machine or your laptop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H &amp;lt;swarm_ip:swarm_port&amp;gt; nodes://&amp;lt;node_ip1:2375&amp;gt;,&amp;lt;node_ip2:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm manage -H &amp;lt;swarm_ip:swarm_port&amp;gt; &amp;lt;node_ip1:2375&amp;gt;,&amp;lt;node_ip2:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the regular Docker commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H &amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;List the nodes in your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm list file:///tmp/my_cluster
&amp;lt;node_ip1:2375&amp;gt;
&amp;lt;node_ip2:2375&amp;gt;
&amp;lt;node_ip3:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;docker-hub-as-a-hosted-discovery-service&#34;&gt;Docker Hub as a hosted discovery service&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: The Docker Hub Hosted Discovery Service &lt;strong&gt;is not recommended&lt;/strong&gt; for production use. It&amp;rsquo;s intended to be used for testing/development. See the  discovery backends for production use.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This example uses the hosted discovery service on Docker Hub. Using
Docker Hub&amp;rsquo;s hosted discovery service requires that each node in the
swarm is connected to the public internet. To create your cluster:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm create
6856663cdefdec325839a4b7e1de38e8 # &amp;lt;- this is your unique &amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create each node and join them to the cluster.&lt;/p&gt;

&lt;p&gt;On each of your nodes, start the swarm agent. The &lt;node_ip&gt; doesn&amp;rsquo;t have to be public (eg. 192.168.0.X) but the the Swarm manager must be able to access it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm join --advertise=&amp;lt;node_ip:2375&amp;gt; token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start the Swarm manager.&lt;/p&gt;

&lt;p&gt;This can be on any machine or even your laptop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swarm manage -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use regular Docker commands to interact with your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; info
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; run ...
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; ps
docker -H tcp://&amp;lt;swarm_ip:swarm_port&amp;gt; logs ...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;List the nodes in your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;swarm list token://&amp;lt;cluster_id&amp;gt;
&amp;lt;node_ip:2375&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;contribute-a-new-discovery-backend&#34;&gt;Contribute a new discovery backend&lt;/h2&gt;

&lt;p&gt;You can contribute a new discovery backend to Swarm. For information on how to
do this, see &lt;a
href=&#34;https://github.com/docker/docker/tree/master/pkg/discovery&#34;&gt;
github.com/docker/docker/pkg/discovery&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;docker-swarm-documentation-index&#34;&gt;Docker Swarm documentation index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/scheduler/strategy/&#34;&gt;Scheduler strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/scheduler/filter/&#34;&gt;Scheduler filters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/swarm-api/&#34;&gt;Swarm API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm</title>
      <link>http://docs-stage.docker.com/swarm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/</guid>
      <description>

&lt;h1 id=&#34;docker-swarm&#34;&gt;Docker Swarm&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/overview/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/get-swarm/&#34;&gt;How to get Docker Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/install-w-machine/&#34;&gt;Evaluate Swarm in a sandbox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/plan-for-production/&#34;&gt;Plan for Swarm in production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/install-manual/&#34;&gt;Build a Swarm cluster for production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/swarm_at_scale/&#34;&gt;Try Swarm at scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/secure-swarm-tls/&#34;&gt;Overview Swarm with TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/configure-tls/&#34;&gt;Configure Docker Swarm for TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/discovery/&#34;&gt;Docker Swarm Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/multi-manager-setup/&#34;&gt;High availability in Docker Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/networking/&#34;&gt;Swarm and container networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/scheduler/&#34;&gt;Advanced Scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/provision-with-machine/&#34;&gt;Provision a Swarm cluster with Docker Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/swarm-api/&#34;&gt;Docker Swarm API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker Swarm API</title>
      <link>http://docs-stage.docker.com/swarm/swarm-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/swarm-api/</guid>
      <description>

&lt;h1 id=&#34;docker-swarm-api&#34;&gt;Docker Swarm API&lt;/h1&gt;

&lt;p&gt;The Docker Swarm API is mostly compatible with the &lt;a href=&#34;https://docs.docker.com/engine/reference/api/docker_remote_api/&#34;&gt;Docker Remote
API&lt;/a&gt;. This
document is an overview of the differences between the Swarm API and the Docker
Remote API.&lt;/p&gt;

&lt;h2 id=&#34;missing-endpoints&#34;&gt;Missing endpoints&lt;/h2&gt;

&lt;p&gt;Some endpoints have not yet been implemented and will return a 404 error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;POST &amp;quot;/images/create&amp;quot; : &amp;quot;docker import&amp;quot; flow not implement
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;endpoints-which-behave-differently&#34;&gt;Endpoints which behave differently&lt;/h2&gt;

&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Endpoint&lt;/th&gt;
        &lt;th&gt;Differences&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;
            &lt;code&gt;GET &#34;/containers/{name:.*}/json&#34;&lt;/code&gt;
        &lt;/td&gt;
        &lt;td&gt;
            New field &lt;code&gt;Node&lt;/code&gt; added:&lt;br /&gt;
&lt;pre&gt;
&#34;Node&#34;: {
    &#34;Id&#34;: &#34;ODAI:IC6Q:MSBL:TPB5:HIEE:6IKC:VCAM:QRNH:PRGX:ERZT:OK46:PMFX&#34;,
    &#34;Ip&#34;: &#34;0.0.0.0&#34;,
    &#34;Addr&#34;: &#34;http://0.0.0.0:4243&#34;,
    &#34;Name&#34;: &#34;vagrant-ubuntu-saucy-64&#34;
}
&lt;/pre&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;
            &lt;code&gt;GET &#34;/containers/{name:.*}/json&#34;&lt;/code&gt;
        &lt;/td&gt;
        &lt;td&gt;
            &lt;code&gt;HostIP&lt;/code&gt; replaced by the the actual Node&#39;s IP if &lt;code&gt;HostIP&lt;/code&gt; is &lt;code&gt;0.0.0.0&lt;/code&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;
            &lt;code&gt;GET &#34;/containers/json&#34;&lt;/code&gt;
        &lt;/td&gt;
        &lt;td&gt;
            Node&#39;s name prepended to the container name.
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;
            &lt;code&gt;GET &#34;/containers/json&#34;&lt;/code&gt;
        &lt;/td&gt;
        &lt;td&gt;
            &lt;code&gt;HostIP&lt;/code&gt; replaced by the the actual Node&#39;s IP if &lt;code&gt;HostIP&lt;/code&gt; is &lt;code&gt;0.0.0.0&lt;/code&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;
            &lt;code&gt;GET &#34;/containers/json&#34;&lt;/code&gt;
        &lt;/td&gt;
        &lt;td&gt;
            Containers started from the &lt;code&gt;swarm&lt;/code&gt; official image are hidden by default, use &lt;code&gt;all=1&lt;/code&gt; to display them.
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;
            &lt;code&gt;GET &#34;/images/json&#34;&lt;/code&gt;
        &lt;/td&gt;
        &lt;td&gt;
            Use &lt;code&gt;--filter node=&amp;lt;Node name&amp;gt;&lt;/code&gt; to show images of the specific node.
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;
            &lt;code&gt;POST &#34;/containers/create&#34;&lt;/code&gt;
        &lt;/td&gt;
        &lt;td&gt;
            &lt;code&gt;CpuShares&lt;/code&gt; in &lt;code&gt;HostConfig&lt;/code&gt; sets the number of CPU cores allocated to the container.
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&#34;registry-authentication&#34;&gt;Registry Authentication&lt;/h2&gt;

&lt;p&gt;During container create calls, the Swarm API will optionally accept an &lt;code&gt;X-Registry-Auth&lt;/code&gt; header.
If provided, this header is passed down to the engine if the image must be pulled
to complete the create operation.&lt;/p&gt;

&lt;p&gt;The following two examples demonstrate how to utilize this using the existing Docker CLI&lt;/p&gt;

&lt;h3 id=&#34;authenticate-using-registry-tokens&#34;&gt;Authenticate using registry tokens&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This example requires Docker Engine 1.10 with auth token support.
For older Engine versions, refer to &lt;a href=&#34;#authenticate-using-username-and-password&#34;&gt;authenticate using username and
password&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This example uses the &lt;a href=&#34;https://stedolan.github.io/jq/&#34;&gt;&lt;code&gt;jq&lt;/code&gt; command-line utility&lt;/a&gt;.
To run this example, install &lt;code&gt;jq&lt;/code&gt; using your package manager (&lt;code&gt;apt-get install jq&lt;/code&gt; or &lt;code&gt;yum install jq&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REPO=yourrepo/yourimage
REPO_USER=yourusername
read -s PASSWORD
AUTH_URL=https://auth.docker.io/token

# obtain a JSON token, and extract the &amp;quot;token&amp;quot; value using &#39;jq&#39;
TOKEN=$(curl -s -u &amp;quot;${REPO_USER}:${PASSWORD}&amp;quot; &amp;quot;${AUTH_URL}?scope=repository:${REPO}:pull&amp;amp;service=registry.docker.io&amp;quot; | jq -r &amp;quot;.token&amp;quot;)
HEADER=$(echo &amp;quot;{\&amp;quot;registrytoken\&amp;quot;:\&amp;quot;${TOKEN}\&amp;quot;}&amp;quot;|base64 -w 0 )
echo HEADER=$HEADER
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add the header you&amp;rsquo;ve calculated to your &lt;code&gt;~/.docker/config.json&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&amp;quot;HttpHeaders&amp;quot;: {
    &amp;quot;X-Registry-Auth&amp;quot;: &amp;quot;&amp;lt;HEADER string from above&amp;gt;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can now authenticate to the registry, and run private images on Swarm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run --rm -it yourprivateimage:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Be aware that tokens are short-lived and will expire quickly.&lt;/p&gt;

&lt;h3 id=&#34;authenticate-using-username-and-password&#34;&gt;Authenticate using username and password&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; this authentication method stores your credentials unencrypted
on the filesystem. Refer to &lt;a href=&#34;#authenticate-using-registry-tokens&#34;&gt;Authenticate using registry tokens&lt;/a&gt;
for a more secure approach.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, calculate the header&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REPO_USER=yourusername
read -s PASSWORD
HEADER=$(echo &amp;quot;{\&amp;quot;username\&amp;quot;:\&amp;quot;${REPO_USER}\&amp;quot;,\&amp;quot;password\&amp;quot;:\&amp;quot;${PASSWORD}\&amp;quot;}&amp;quot; | base64 -w 0 )
unset PASSWORD
echo HEADER=$HEADER
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add the header you&amp;rsquo;ve calculated to your &lt;code&gt;~/.docker/config.json&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&amp;quot;HttpHeaders&amp;quot;: {
    &amp;quot;X-Registry-Auth&amp;quot;: &amp;quot;&amp;lt;HEADER string from above&amp;gt;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can now authenticate to the registry, and run private images on Swarm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run --rm -it yourprivateimage:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;docker-swarm-documentation-index&#34;&gt;Docker Swarm documentation index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/discovery/&#34;&gt;Discovery options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/scheduler/strategy/&#34;&gt;Scheduler strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/swarm/scheduler/filter/&#34;&gt;Scheduler filters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Evaluate Swarm in a sandbox</title>
      <link>http://docs-stage.docker.com/swarm/install-w-machine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/install-w-machine/</guid>
      <description>

&lt;h1 id=&#34;evaluate-swarm-in-a-sandbox&#34;&gt;Evaluate Swarm in a sandbox&lt;/h1&gt;

&lt;p&gt;This getting started example shows you how to create a Docker Swarm, the
native clustering tool for Docker.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll use Docker Toolbox to install Docker Machine and some other tools on your computer. Then you&amp;rsquo;ll use Docker Machine to provision a set of Docker Engine hosts. Lastly, you&amp;rsquo;ll use Docker client to connect to the hosts, where you&amp;rsquo;ll create a discovery token, create a cluster of one Swarm manager and nodes, and manage the cluster.&lt;/p&gt;

&lt;p&gt;When you finish, you&amp;rsquo;ll have a Docker Swarm up and running in VirtualBox on your
local Mac or Windows computer. You can use this Swarm as personal development
sandbox.&lt;/p&gt;

&lt;p&gt;To use Docker Swarm on Linux, see &lt;a href=&#34;../swarm/install-manual/&#34;&gt;Build a Swarm cluster for production&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;install-docker-toolbox&#34;&gt;Install Docker Toolbox&lt;/h2&gt;

&lt;p&gt;Download and install &lt;a href=&#34;https://www.docker.com/docker-toolbox&#34;&gt;Docker Toolbox&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The toolbox installs a handful of tools on your local Windows or Mac OS X computer. In this exercise, you use three of those tools:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker Machine: To deploy virtual machines that run Docker Engine.&lt;/li&gt;
&lt;li&gt;VirtualBox: To host the virtual machines deployed from Docker Machine.&lt;/li&gt;
&lt;li&gt;Docker Client: To connect from your local computer to the Docker Engines on the VMs and issue docker commands to create the Swarm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following sections provide more information on each of these tools. The rest of the document uses the abbreviation, VM, for virtual machine.&lt;/p&gt;

&lt;h2 id=&#34;create-three-vms-running-docker-engine&#34;&gt;Create three VMs running Docker Engine&lt;/h2&gt;

&lt;p&gt;Here, you use Docker Machine to provision three VMs running Docker Engine.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a terminal on your computer. Use Docker Machine to list any VMs in VirtualBox.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine ls
NAME         ACTIVE   DRIVER       STATE     URL                         SWARM
default    *        virtualbox   Running   tcp://192.168.99.100:2376
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Optional: To conserve system resources, stop any virtual machines you are not using. For example, to stop the VM named &lt;code&gt;default&lt;/code&gt;, enter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine stop default
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create and run a VM named &lt;code&gt;manager&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox manager
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create and run a VM named &lt;code&gt;agent1&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox agent1
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create and run a VM named &lt;code&gt;agent2&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox agent2
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each create command checks for a local copy of the &lt;em&gt;latest&lt;/em&gt; VM image, called boot2docker.iso. If it isn&amp;rsquo;t available, Docker Machine downloads the image from Docker Hub. Then, Docker Machine uses boot2docker.iso to create a VM that automatically runs Docker Engine.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Troubleshooting: If your computer or hosts cannot reach Docker Hub, the
&lt;code&gt;docker-machine&lt;/code&gt; or &lt;code&gt;docker run&lt;/code&gt; commands that pull images may fail. In that
case, check the &lt;a href=&#34;http://status.docker.com/&#34;&gt;Docker Hub status page&lt;/a&gt; for
service availability. Then, check whether your computer is connected to the Internet.  Finally, check whether VirtualBox&amp;rsquo;s network settings allow your hosts to connect to the Internet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;create-a-swarm-discovery-token&#34;&gt;Create a Swarm discovery token&lt;/h2&gt;

&lt;p&gt;Here you use the discovery backend hosted on Docker Hub to create a unique discovery token for your cluster. This discovery backend is only for low-volume development and testing purposes, not for production. Later on, when you run the Swarm manager and nodes, they register with the discovery backend as members of the cluster that&amp;rsquo;s associated with the unique token. The discovery backend maintains an up-to-date list of cluster members and shares that list with the Swarm manager. The Swarm manager uses this list to assign tasks to the nodes.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Connect the Docker Client on your computer to the Docker Engine running on &lt;code&gt;manager&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval $(docker-machine env manager)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The client will send the &lt;code&gt;docker&lt;/code&gt; commands in the following steps to the Docker Engine on &lt;code&gt;manager&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a unique id for the Swarm cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run --rm swarm create
.
.
.
Status: Downloaded newer image for swarm:latest
0ac50ef75c9739f5bfeeaf00503d4e6e
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;docker run&lt;/code&gt; command gets the latest &lt;code&gt;swarm&lt;/code&gt; image and runs it as a container. The &lt;code&gt;create&lt;/code&gt; argument makes the Swarm container connect to the Docker Hub discovery service and get a unique Swarm ID, also known as a &amp;ldquo;discovery token&amp;rdquo;. The token appears in the output, it is not saved to a file on the host. The &lt;code&gt;--rm&lt;/code&gt; option automatically cleans up the container and removes the file system when the container exits.&lt;/p&gt;

&lt;p&gt;The discovery service keeps unused tokens for approximately one week.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Copy the discovery token from the last line of the previous output to a safe place.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;create-the-swarm-manager-and-nodes&#34;&gt;Create the Swarm manager and nodes&lt;/h2&gt;

&lt;p&gt;Here, you connect to each of the hosts and create a Swarm manager or node.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;List the VMs to check that they&amp;rsquo;re set up and running. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine ls
NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER   ERRORS
agent1    -        virtualbox   Running   tcp://192.168.99.102:2376           v1.9.1
agent2    -        virtualbox   Running   tcp://192.168.99.103:2376           v1.9.1
manager   *        virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Your client should still be pointing to Docker Engine on &lt;code&gt;manager&lt;/code&gt;. Use the following syntax to run a Swarm container that functions as the primary manager on &lt;code&gt;manager&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p &amp;lt;your_selected_port&amp;gt;:3376 -t -v /var/lib/boot2docker:/certs:ro swarm manage -H 0.0.0.0:3376 --tlsverify --tlscacert=/certs/ca.pem --tlscert=/certs/server.pem --tlskey=/certs/server-key.pem token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 3376:3376 -t -v /var/lib/boot2docker:/certs:ro swarm manage -H 0.0.0.0:3376 --tlsverify --tlscacert=/certs/ca.pem --tlscert=/certs/server.pem --tlskey=/certs/server-key.pem token://0ac50ef75c9739f5bfeeaf00503d4e6e
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-p&lt;/code&gt; option maps a port 3376 on the container to port 3376 on the host. The &lt;code&gt;-v&lt;/code&gt; option mounts the directory containing TLS certificates (&lt;code&gt;/var/lib/boot2docker&lt;/code&gt; for the &lt;code&gt;manager&lt;/code&gt; VM) into the container running Swarm manager in read-only mode.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connect Docker Client to &lt;code&gt;agent1&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval $(docker-machine env agent1)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the following syntax to run a Swarm container that functions as an agent on &lt;code&gt;agent1&lt;/code&gt;. Replace &lt;code&gt;&amp;lt;node_ip&amp;gt;&lt;/code&gt; with the IP address of the VM from above, or use the &lt;code&gt;docker-machine ip&lt;/code&gt; command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d swarm join --addr=&amp;lt;node_ip&amp;gt;:&amp;lt;node_port&amp;gt; token://&amp;lt;cluster_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d swarm join --addr=$(docker-machine ip agent1):2376 token://0ac50ef75c9739f5bfeeaf00503d4e6e
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connect Docker Client to &lt;code&gt;agent2&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval $(docker-machine env agent2)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run a Swarm container as an agent on &lt;code&gt;agent2&lt;/code&gt;. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d swarm join --addr=$(docker-machine ip agent2):2376 token://0ac50ef75c9739f5bfeeaf00503d4e6e
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;manage-your-swarm&#34;&gt;Manage your Swarm&lt;/h2&gt;

&lt;p&gt;Here, you connect to the cluster and review information about the Swarm manager and nodes. You tell the Swarm to run a container and check which node did the work.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Connect the Docker Client to the Swarm by updating the &lt;code&gt;DOCKER_HOST&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ DOCKER_HOST=&amp;lt;manager_ip&amp;gt;:&amp;lt;your_selected_port&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use the &lt;code&gt;docker-machine ip&lt;/code&gt; command, and we selected port 3376 for the Swarm manager.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ DOCKER_HOST=$(docker-machine ip manager):3376
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because Docker Swarm uses the standard Docker API, you can connect to it using Docker Client and other tools such as Docker Compose, Dokku, Jenkins, and Krane, among others.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get information about the Swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the output displays information about the two agent nodes and the one manager node in the Swarm.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check the images currently running on your Swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run a container on the Swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run hello-world
Hello from Docker.
.
.
.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;docker ps&lt;/code&gt; command to find out which node the container ran on. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES
0b0628349187        hello-world         &amp;quot;/hello&amp;quot;                 20 minutes ago      Exited (0) 11 seconds ago                       agent1
.
.
.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, the Swarm ran &lt;code&gt;hello-world&lt;/code&gt; on &lt;code&gt;agent1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;By default, Docker Swarm uses the &amp;ldquo;spread&amp;rdquo; strategy to choose which node runs a container. When you run multiple containers, the spread strategy assigns each container to the node with the fewest containers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;where-to-go-next&#34;&gt;Where to go next&lt;/h2&gt;

&lt;p&gt;At this point, you&amp;rsquo;ve done the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Created a Swarm discovery token.&lt;/li&gt;
&lt;li&gt;Created Swarm nodes using Docker Machine.&lt;/li&gt;
&lt;li&gt;Managed a Swarm cluster and run containers on it.&lt;/li&gt;
&lt;li&gt;Learned Swarm-related concepts and terminology.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, Docker Swarm has many other aspects and capabilities.
For more information, visit &lt;a href=&#34;https://www.docker.com/docker-swarm&#34;&gt;the Swarm landing page&lt;/a&gt; or read the &lt;a href=&#34;https://docs.docker.com/swarm/&#34;&gt;Swarm documentation&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Filters</title>
      <link>http://docs-stage.docker.com/swarm/scheduler/filter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/scheduler/filter/</guid>
      <description>

&lt;h1 id=&#34;swarm-filters&#34;&gt;Swarm filters&lt;/h1&gt;

&lt;p&gt;Filters tell Docker Swarm scheduler which nodes to use when creating and running
a container.&lt;/p&gt;

&lt;h2 id=&#34;configure-the-available-filters&#34;&gt;Configure the available filters&lt;/h2&gt;

&lt;p&gt;Filters are divided into two categories, node filters and container configuration
filters. Node filters operate on characteristics of the Docker host or on the
configuration of the Docker daemon. Container configuration filters operate on
characteristics of containers, or on the availability of images on a host.&lt;/p&gt;

&lt;p&gt;Each filter has a name that identifies it. The node filters are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;constraint&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;health&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;containerslots&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The container configuration filters are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;affinity&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dependency&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;port&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When you start a Swarm manager with the &lt;code&gt;swarm manage&lt;/code&gt; command, all the filters
are enabled. If you want to limit the filters available to your Swarm, specify a subset
of filters by passing the &lt;code&gt;--filter&lt;/code&gt; flag and the name:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ swarm manage --filter=health --filter=dependency
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Container configuration filters match all containers, including stopped
containers, when applying the filter. To release a node used by a container, you
must remove the container from the node.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;node-filters&#34;&gt;Node filters&lt;/h2&gt;

&lt;p&gt;When creating a container or building an image, you use a &lt;code&gt;constraint&lt;/code&gt; or
&lt;code&gt;health&lt;/code&gt; filter to select a subset of nodes to consider for scheduling.
If a node in Swarm cluster has a label with key &lt;code&gt;containerslots&lt;/code&gt;
and a number-value, Swarm will not launch more containers than the given number.&lt;/p&gt;

&lt;h3 id=&#34;use-a-constraint-filter&#34;&gt;Use a constraint filter&lt;/h3&gt;

&lt;p&gt;Node constraints can refer to Docker&amp;rsquo;s default tags or to custom labels. Default
tags are sourced from &lt;code&gt;docker info&lt;/code&gt;. Often, they relate to properties of the Docker
host. Currently, the default tags include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node&lt;/code&gt; to refer to the node by ID or name&lt;/li&gt;
&lt;li&gt;&lt;code&gt;storagedriver&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;executiondriver&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kernelversion&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;operatingsystem&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Custom node labels you apply when you start the &lt;code&gt;docker daemon&lt;/code&gt;, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker daemon --label com.example.environment=&amp;quot;production&amp;quot; --label
com.example.storage=&amp;quot;ssd&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, when you start a container on the cluster, you can set constraints using
these default tags or custom labels. The Swarm scheduler looks for matching node
on the cluster and starts the container there. This approach has several
practical applications:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Schedule based on specific host properties, for example,&lt;code&gt;storage=ssd&lt;/code&gt; schedules
containers on specific hardware.&lt;/li&gt;
&lt;li&gt;Force containers to run in a given location, for example region=us-east`.&lt;/li&gt;
&lt;li&gt;Create logical cluster partitions by splitting a cluster into
sub-clusters with different properties, for example &lt;code&gt;environment=production&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;example-node-constraints&#34;&gt;Example node constraints&lt;/h4&gt;

&lt;p&gt;To specify custom label for a node, pass a list of &lt;code&gt;--label&lt;/code&gt;
options at &lt;code&gt;docker&lt;/code&gt; startup time. For instance, to start &lt;code&gt;node-1&lt;/code&gt; with the
&lt;code&gt;storage=ssd&lt;/code&gt; label:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker daemon --label storage=ssd
$ swarm join --advertise=192.168.0.42:2375 token://XXXXXXXXXXXXXXXXXX
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You might start a different &lt;code&gt;node-2&lt;/code&gt; with &lt;code&gt;storage=disk&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker daemon --label storage=disk
$ swarm join --advertise=192.168.0.43:2375 token://XXXXXXXXXXXXXXXXXX
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the nodes are joined to a cluster, the Swarm manager pulls their respective
tags.  Moving forward, the manager takes the tags into account when scheduling
new containers.&lt;/p&gt;

&lt;p&gt;Continuing the previous example, assuming your cluster with &lt;code&gt;node-1&lt;/code&gt; and
&lt;code&gt;node-2&lt;/code&gt;, you can run a MySQL server container on the cluster.  When you run the
container, you can use a &lt;code&gt;constraint&lt;/code&gt; to ensure the database gets good I/O
performance. You do this by filtering for nodes with flash drives:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt;  run -d -P -e constraint:storage==ssd --name db mysql
f8b693db9cd6

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt;  ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES
f8b693db9cd6        mysql:latest        &amp;quot;mysqld&amp;quot;            Less than a second ago   running             192.168.0.42:49178-&amp;gt;3306/tcp    node-1/db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, the manager selected all nodes that met the &lt;code&gt;storage=ssd&lt;/code&gt;
constraint and applied resource management on top of them.   Only &lt;code&gt;node-1&lt;/code&gt; was
selected because it&amp;rsquo;s the only host running flash.&lt;/p&gt;

&lt;p&gt;Suppose you want to run an Nginx frontend in a cluster. In this case, you wouldn&amp;rsquo;t want flash drives because the frontend mostly writes logs to disk.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -P -e constraint:storage==disk --name frontend nginx
963841b138d8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES
963841b138d8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.43:49177-&amp;gt;80/tcp      node-2/frontend
f8b693db9cd6        mysql:latest        &amp;quot;mysqld&amp;quot;            Up About a minute        running             192.168.0.42:49178-&amp;gt;3306/tcp    node-1/db
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The scheduler selected &lt;code&gt;node-2&lt;/code&gt; since it was started with the &lt;code&gt;storage=disk&lt;/code&gt; label.&lt;/p&gt;

&lt;p&gt;Finally, build args can be used to apply node constraints to a &lt;code&gt;docker build&lt;/code&gt;.
Again, you&amp;rsquo;ll avoid flash drives.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir sinatra
$ cd sinatra
$ echo &amp;quot;FROM ubuntu:14.04&amp;quot; &amp;gt; Dockerfile
$ echo &amp;quot;MAINTAINER Kate Smith &amp;lt;ksmith@example.com&amp;gt;&amp;quot; &amp;gt;&amp;gt; Dockerfile
$ echo &amp;quot;RUN apt-get update &amp;amp;&amp;amp; apt-get install -y ruby ruby-dev&amp;quot; &amp;gt;&amp;gt; Dockerfile
$ echo &amp;quot;RUN gem install sinatra&amp;quot; &amp;gt;&amp;gt; Dockerfile
$ docker build --build-arg=constraint:storage==disk -t ouruser/sinatra:v2 .
Sending build context to Docker daemon 2.048 kB
Step 1 : FROM ubuntu:14.04
 ---&amp;gt; a5a467fddcb8
Step 2 : MAINTAINER Kate Smith &amp;lt;ksmith@example.com&amp;gt;
 ---&amp;gt; Running in 49e97019dcb8
 ---&amp;gt; de8670dcf80e
Removing intermediate container 49e97019dcb8
Step 3 : RUN apt-get update &amp;amp;&amp;amp; apt-get install -y ruby ruby-dev
 ---&amp;gt; Running in 26c9fbc55aeb
 ---&amp;gt; 30681ef95fff
Removing intermediate container 26c9fbc55aeb
Step 4 : RUN gem install sinatra
 ---&amp;gt; Running in 68671d4a17b0
 ---&amp;gt; cd70495a1514
Removing intermediate container 68671d4a17b0
Successfully built cd70495a1514

$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
dockerswarm/swarm   manager             8c2c56438951        2 days ago          795.7 MB
ouruser/sinatra     v2                  cd70495a1514        35 seconds ago      318.7 MB
ubuntu              14.04               a5a467fddcb8        11 days ago         187.9 MB
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;use-the-health-filter&#34;&gt;Use the health filter&lt;/h3&gt;

&lt;p&gt;The node &lt;code&gt;health&lt;/code&gt; filter prevents the scheduler form running containers
on unhealthy nodes. A node is considered unhealthy if the node is down or it
can&amp;rsquo;t communicate with the cluster store.&lt;/p&gt;

&lt;h3 id=&#34;use-the-containerslots-filter&#34;&gt;Use the containerslots filter&lt;/h3&gt;

&lt;p&gt;You may give your Docker nodes the containerslots label&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker daemon --label containerslots=3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Swarm will run up to 3 containers at this node, if all nodes are &amp;ldquo;full&amp;rdquo;,
an error is thrown indicating no suitable node can be found.
If the value is not castable to an integer number or is not present,
there will be no limit on container number.&lt;/p&gt;

&lt;h2 id=&#34;container-filters&#34;&gt;Container filters&lt;/h2&gt;

&lt;p&gt;When creating a container, you can use three types of container filters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#use-an-affinity-filter&#34;&gt;&lt;code&gt;affinity&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#use-a-depedency-filter&#34;&gt;&lt;code&gt;dependency&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#use-a-port-filter&#34;&gt;&lt;code&gt;port&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;use-an-affinity-filter&#34;&gt;Use an affinity filter&lt;/h3&gt;

&lt;p&gt;Use an &lt;code&gt;affinity&lt;/code&gt; filter to create &amp;ldquo;attractions&amp;rdquo; between containers. For
example, you can run a container and instruct Swarm to schedule it next to
another container based on these affinities:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;container name or id&lt;/li&gt;
&lt;li&gt;an image on the host&lt;/li&gt;
&lt;li&gt;a custom label applied to the container&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These affinities ensure that containers run on the same network node
&amp;mdash; without you having to know what each node is running.&lt;/p&gt;

&lt;h4 id=&#34;example-name-affinity&#34;&gt;Example name affinity&lt;/h4&gt;

&lt;p&gt;You can schedule a new container to run next to another based on a container
name or ID. For example, you can start a container called &lt;code&gt;frontend&lt;/code&gt; running
&lt;code&gt;nginx&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt;  run -d -p 80:80 --name frontend nginx
 87c4376856a8


$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1/frontend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, using &lt;code&gt;-e affinity:container==frontend&lt;/code&gt; value to schedule a second
container to locate and run next to the container named &lt;code&gt;frontend&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name logger -e affinity:container==frontend logger
 87c4376856a8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1/frontend
963841b138d8        logger:latest       &amp;quot;logger&amp;quot;            Less than a second ago   running                                             node-1/logger
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because of &lt;code&gt;name&lt;/code&gt; affinity, the  &lt;code&gt;logger&lt;/code&gt; container ends up on &lt;code&gt;node-1&lt;/code&gt; along
with the &lt;code&gt;frontend&lt;/code&gt; container. Instead of the &lt;code&gt;frontend&lt;/code&gt; name you could have
supplied its ID as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name logger -e affinity:container==87c4376856a8
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;example-image-affinity&#34;&gt;Example image affinity&lt;/h4&gt;

&lt;p&gt;You can schedule a container to run only on nodes where a specific image is
already pulled. For example, suppose you pull a &lt;code&gt;redis&lt;/code&gt; image to two hosts and a
&lt;code&gt;mysql&lt;/code&gt; image to a third.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker -H node-1:2375 pull redis
$ docker -H node-2:2375 pull mysql
$ docker -H node-3:2375 pull redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Only &lt;code&gt;node-1&lt;/code&gt; and &lt;code&gt;node-3&lt;/code&gt; have the &lt;code&gt;redis&lt;/code&gt; image. Specify a &lt;code&gt;-e
affinity:image==redis&lt;/code&gt; filter to schedule several additional containers to run
on these nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis1 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis2 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis3 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis4 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis5 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis6 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis7 -e affinity:image==redis redis
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis8 -e affinity:image==redis redis

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES
87c4376856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1/redis1
1212386856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1/redis2
87c4376639a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3/redis3
1234376856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1/redis4
86c2136253a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3/redis5
87c3236856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3/redis6
87c4376856a8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-3/redis7
963841b138d8        redis:latest        &amp;quot;redis&amp;quot;             Less than a second ago   running                                             node-1/redis8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see here, the containers were only scheduled on nodes that had the
&lt;code&gt;redis&lt;/code&gt; image. Instead of the image name, you could have specified the image ID.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker images
REPOSITORY                         TAG                       IMAGE ID            CREATED             VIRTUAL SIZE
redis                              latest                    06a1f75304ba        2 days ago          111.1 MB

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis1 -e affinity:image==06a1f75304ba redis
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;example-label-affinity&#34;&gt;Example label affinity&lt;/h4&gt;

&lt;p&gt;A label affinity allows you to filter based on a custom container label. For
example, you can run a &lt;code&gt;nginx&lt;/code&gt; container and apply the
&lt;code&gt;com.example.type=frontend&lt;/code&gt; custom label.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 80:80 --label com.example.type=frontend nginx
 87c4376856a8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps  --filter &amp;quot;label=com.example.type=frontend&amp;quot;
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1/trusting_yonath
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, use &lt;code&gt;-e affinity:com.example.type==frontend&lt;/code&gt; to schedule a container next
to the container with the &lt;code&gt;com.example.type==frontend&lt;/code&gt; label.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -e affinity:com.example.type==frontend logger
 87c4376856a8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                           NAMES
87c4376856a8        nginx:latest        &amp;quot;nginx&amp;quot;             Less than a second ago   running             192.168.0.42:80-&amp;gt;80/tcp         node-1/trusting_yonath
963841b138d8        logger:latest       &amp;quot;logger&amp;quot;            Less than a second ago   running                                             node-1/happy_hawking
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;logger&lt;/code&gt; container ends up on &lt;code&gt;node-1&lt;/code&gt; because its affinity with the
&lt;code&gt;com.example.type==frontend&lt;/code&gt; label.&lt;/p&gt;

&lt;h3 id=&#34;use-a-dependency-filter&#34;&gt;Use a dependency filter&lt;/h3&gt;

&lt;p&gt;A container dependency filter co-schedules dependent containers on the same node.
Currently, dependencies are declared as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--volumes-from=dependency&lt;/code&gt; (shared volumes)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--link=dependency:alias&lt;/code&gt; (links)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--net=container:dependency&lt;/code&gt; (shared network stacks)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Swarm attempts to co-locate the dependent container on the same node. If it
cannot be done (because the dependent container doesn&amp;rsquo;t exist, or because the
node doesn&amp;rsquo;t have enough resources), it will prevent the container creation.&lt;/p&gt;

&lt;p&gt;The combination of multiple dependencies are honored if possible. For
instance, if you specify &lt;code&gt;--volumes-from=A --net=container:B&lt;/code&gt;,  the scheduler
attempts to co-locate the container on the same node as &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. If those
containers are running on different nodes, Swarm does not schedule the container.&lt;/p&gt;

&lt;h3 id=&#34;use-a-port-filter&#34;&gt;Use a port filter&lt;/h3&gt;

&lt;p&gt;When the &lt;code&gt;port&lt;/code&gt; filter is enabled, a container&amp;rsquo;s port configuration is used as a
unique constraint. Docker Swarm selects a node where a particular port is
available and unoccupied by another container or process. Required ports may be
specified by mapping a host port, or using the host networking and exposing a
port using the container configuration.&lt;/p&gt;

&lt;h4 id=&#34;example-in-bridge-mode&#34;&gt;Example in bridge mode&lt;/h4&gt;

&lt;p&gt;By default, containers run on Docker&amp;rsquo;s bridge network. To use the &lt;code&gt;port&lt;/code&gt; filter
with the bridge network, you run a container as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 80:80 nginx
87c4376856a8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID    IMAGE               COMMAND         PORTS                       NAMES
87c4376856a8    nginx:latest        &amp;quot;nginx&amp;quot;         192.168.0.42:80-&amp;gt;80/tcp     node-1/prickly_engelbart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Docker Swarm selects a node where port &lt;code&gt;80&lt;/code&gt; is available and unoccupied by another
container or process, in this case &lt;code&gt;node-1&lt;/code&gt;. Attempting to run another container
that uses the host port &lt;code&gt;80&lt;/code&gt; results in Swarm selecting a different node,
because port &lt;code&gt;80&lt;/code&gt; is already occupied on &lt;code&gt;node-1&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 80:80 nginx
963841b138d8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE          COMMAND        PORTS                           NAMES
963841b138d8        nginx:latest   &amp;quot;nginx&amp;quot;        192.168.0.43:80-&amp;gt;80/tcp         node-2/dreamy_turing
87c4376856a8        nginx:latest   &amp;quot;nginx&amp;quot;        192.168.0.42:80-&amp;gt;80/tcp         node-1/prickly_engelbart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, repeating the same command will result in the selection of &lt;code&gt;node-3&lt;/code&gt;,
since port &lt;code&gt;80&lt;/code&gt; is neither available on &lt;code&gt;node-1&lt;/code&gt; nor &lt;code&gt;node-2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 80:80 nginx
963841b138d8

$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID   IMAGE               COMMAND        PORTS                           NAMES
f8b693db9cd6   nginx:latest        &amp;quot;nginx&amp;quot;        192.168.0.44:80-&amp;gt;80/tcp         node-3/stoic_albattani
963841b138d8   nginx:latest        &amp;quot;nginx&amp;quot;        192.168.0.43:80-&amp;gt;80/tcp         node-2/dreamy_turing
87c4376856a8   nginx:latest        &amp;quot;nginx&amp;quot;        192.168.0.42:80-&amp;gt;80/tcp         node-1/prickly_engelbart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, Docker Swarm will refuse to run another container that requires port
&lt;code&gt;80&lt;/code&gt;, because it is not available on any node in the cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 80:80 nginx
2014/10/29 00:33:20 Error response from daemon: no resources available to schedule container
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each container occupies port &lt;code&gt;80&lt;/code&gt; on its residing node when the container
is created and releases the port when the container is deleted. A container in &lt;code&gt;exited&lt;/code&gt;
state still owns the port. If &lt;code&gt;prickly_engelbart&lt;/code&gt; on &lt;code&gt;node-1&lt;/code&gt; is stopped but not
deleted, trying to start another container on &lt;code&gt;node-1&lt;/code&gt; that requires port &lt;code&gt;80&lt;/code&gt; would fail
because port &lt;code&gt;80&lt;/code&gt; is associated with &lt;code&gt;prickly_engelbart&lt;/code&gt;. To increase running
instances of nginx, you can either restart &lt;code&gt;prickly_engelbart&lt;/code&gt;, or start another container
after deleting &lt;code&gt;prickly_englbart&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;node-port-filter-with-host-networking&#34;&gt;Node port filter with host networking&lt;/h4&gt;

&lt;p&gt;A container running with &lt;code&gt;--net=host&lt;/code&gt; differs from the default
&lt;code&gt;bridge&lt;/code&gt; mode as the &lt;code&gt;host&lt;/code&gt; mode does not perform any port binding. Instead,
host mode requires that you  explicitly expose one or more port numbers.  You
expose a port using &lt;code&gt;EXPOSE&lt;/code&gt; in the &lt;code&gt;Dockerfile&lt;/code&gt; or &lt;code&gt;--expose&lt;/code&gt; on the command
line. Swarm makes use of this information in conjunction with the &lt;code&gt;host&lt;/code&gt; mode to
choose an available node for a new container.&lt;/p&gt;

&lt;p&gt;For example, the following commands start &lt;code&gt;nginx&lt;/code&gt; on 3-node cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --expose=80 --net=host nginx
640297cb29a7
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --expose=80 --net=host nginx
7ecf562b1b3f
$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --expose=80 --net=host nginx
09a92f582bc2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Port binding information is not available through the &lt;code&gt;docker ps&lt;/code&gt; command because
all the nodes were started with the &lt;code&gt;host&lt;/code&gt; network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND                CREATED                  STATUS              PORTS               NAMES
640297cb29a7        nginx:1             &amp;quot;nginx -g &#39;daemon of   Less than a second ago   Up 30 seconds                           box3/furious_heisenberg
7ecf562b1b3f        nginx:1             &amp;quot;nginx -g &#39;daemon of   Less than a second ago   Up 28 seconds                           box2/ecstatic_meitner
09a92f582bc2        nginx:1             &amp;quot;nginx -g &#39;daemon of   46 seconds ago           Up 27 seconds                           box1/mad_goldstine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Swarm refuses the operation when trying to instantiate the 4th container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$  docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --expose=80 --net=host nginx
FATA[0000] Error response from daemon: unable to find a node with port 80/tcp available in the Host mode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, port binding to the different value, for example  &lt;code&gt;81&lt;/code&gt;, is still allowed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$  docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d -p 81:80 nginx:latest
832f42819adc
$  docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; ps
CONTAINER ID        IMAGE               COMMAND                CREATED                  STATUS                  PORTS                                 NAMES
832f42819adc        nginx:1             &amp;quot;nginx -g &#39;daemon of   Less than a second ago   Up Less than a second   443/tcp, 192.168.136.136:81-&amp;gt;80/tcp   box3/thirsty_hawking
640297cb29a7        nginx:1             &amp;quot;nginx -g &#39;daemon of   8 seconds ago            Up About a minute                                             box3/furious_heisenberg
7ecf562b1b3f        nginx:1             &amp;quot;nginx -g &#39;daemon of   13 seconds ago           Up About a minute                                             box2/ecstatic_meitner
09a92f582bc2        nginx:1             &amp;quot;nginx -g &#39;daemon of   About a minute ago       Up About a minute                                             box1/mad_goldstine
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-to-write-filter-expressions&#34;&gt;How to write filter expressions&lt;/h2&gt;

&lt;p&gt;To apply a node &lt;code&gt;constraint&lt;/code&gt; or container &lt;code&gt;affinity&lt;/code&gt; filters you must set
environment variables on the container using filter expressions, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker tcp://&amp;lt;manager_ip:manager_port&amp;gt; run -d --name redis1 -e affinity:image==~redis redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each expression must be in the form:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;filter-type&amp;gt;:&amp;lt;key&amp;gt;&amp;lt;operator&amp;gt;&amp;lt;value&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;filter-type&amp;gt;&lt;/code&gt; is either the &lt;code&gt;affinity&lt;/code&gt; or the &lt;code&gt;constraint&lt;/code&gt; keyword. It
identifies the type filter you intend to use.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;key&amp;gt;&lt;/code&gt; is an alpha-numeric and must start with a letter or underscore. The
&lt;code&gt;&amp;lt;key&amp;gt;&lt;/code&gt; corresponds to one of the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the &lt;code&gt;container&lt;/code&gt; keyword&lt;/li&gt;
&lt;li&gt;the &lt;code&gt;node&lt;/code&gt; keyword&lt;/li&gt;
&lt;li&gt;a default tag (node constraints)&lt;/li&gt;
&lt;li&gt;a custom metadata label (nodes or containers).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;operator&amp;gt;&lt;/code&gt;is either &lt;code&gt;==&lt;/code&gt; or &lt;code&gt;!=&lt;/code&gt;. By default, expression operators are
hard enforced. If an expression is not met exactly , the manager does not
schedule the container. You can use a &lt;code&gt;~&lt;/code&gt;(tilde) to create a &amp;ldquo;soft&amp;rdquo; expression.
The scheduler tries to match a soft expression. If the expression is not met,
the scheduler discards the filter and schedules the container according to the
scheduler&amp;rsquo;s strategy.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;value&amp;gt;&lt;/code&gt; is an alpha-numeric string, dots, hyphens, and underscores making
up one of the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A globbing pattern, for example, &lt;code&gt;abc*&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A regular expression in the form of &lt;code&gt;/regexp/&lt;/code&gt;. See
&lt;a href=&#34;https://github.com/google/re2/wiki/Syntax&#34;&gt;re2 syntax&lt;/a&gt; for the supported
regex syntax.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following examples illustrate some possible expressions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;constraint:node==node1&lt;/code&gt; matches node &lt;code&gt;node1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node!=node1&lt;/code&gt; matches all nodes, except &lt;code&gt;node1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:region!=us*&lt;/code&gt; matches all nodes outside with a &lt;code&gt;region&lt;/code&gt; tag prefixed with &lt;code&gt;us&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node==/node[12]/&lt;/code&gt; matches nodes &lt;code&gt;node1&lt;/code&gt; and &lt;code&gt;node2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node==/node\d/&lt;/code&gt; matches all nodes with &lt;code&gt;node&lt;/code&gt; + 1 digit.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node!=/node-[01]/&lt;/code&gt; matches all nodes, except &lt;code&gt;node-0&lt;/code&gt; and &lt;code&gt;node-1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node!=/foo\[bar\]/&lt;/code&gt; matches all nodes, except &lt;code&gt;foo[bar]&lt;/code&gt;. You can see the use of escape characters here.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:node==/(?i)node1/&lt;/code&gt; matches node &lt;code&gt;node1&lt;/code&gt; case-insensitive. So &lt;code&gt;NoDe1&lt;/code&gt; or &lt;code&gt;NODE1&lt;/code&gt; also match.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;affinity:image==~redis&lt;/code&gt; tries to match for nodes running container with a &lt;code&gt;redis&lt;/code&gt; image&lt;/li&gt;
&lt;li&gt;&lt;code&gt;constraint:region==~us*&lt;/code&gt; searches for nodes in the cluster belonging to the &lt;code&gt;us&lt;/code&gt; region&lt;/li&gt;
&lt;li&gt;&lt;code&gt;affinity:container!=~redis*&lt;/code&gt; schedule a new &lt;code&gt;redis5&lt;/code&gt; container to a node
without a container that satisfies &lt;code&gt;redis*&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;related-information&#34;&gt;Related information&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/&#34;&gt;Docker Swarm overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/discovery/&#34;&gt;Discovery options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/scheduler/strategy/&#34;&gt;Scheduler strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/swarm-api/&#34;&gt;Swarm API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>High availability in Swarm</title>
      <link>http://docs-stage.docker.com/swarm/multi-manager-setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/multi-manager-setup/</guid>
      <description>

&lt;h1 id=&#34;high-availability-in-docker-swarm&#34;&gt;High availability in Docker Swarm&lt;/h1&gt;

&lt;p&gt;In Docker Swarm, the &lt;strong&gt;Swarm manager&lt;/strong&gt; is responsible for the entire cluster and manages the resources of multiple &lt;em&gt;Docker hosts&lt;/em&gt; at scale. If the Swarm manager dies, you must create a new one and deal with an interruption of service.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;High Availability&lt;/em&gt; feature allows a Docker Swarm to gracefully handle the failover of a manager instance. Using this feature, you can create a single &lt;strong&gt;primary manager&lt;/strong&gt; instance and multiple &lt;strong&gt;replica&lt;/strong&gt; instances.&lt;/p&gt;

&lt;p&gt;A primary manager is the main point of contact with the Docker Swarm cluster. You can also create and talk to replica instances that will act as backups. Requests issued on a replica are automatically proxied to the primary manager. If the primary manager fails, a replica takes away the lead. In this way, you always keep a point of contact with the cluster.&lt;/p&gt;

&lt;h2 id=&#34;setup-primary-and-replicas&#34;&gt;Setup primary and replicas&lt;/h2&gt;

&lt;p&gt;This section explains how to set up Docker Swarm using multiple &lt;strong&gt;managers&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;assumptions&#34;&gt;Assumptions&lt;/h3&gt;

&lt;p&gt;You need either a &lt;code&gt;Consul&lt;/code&gt;, &lt;code&gt;etcd&lt;/code&gt;, or &lt;code&gt;Zookeeper&lt;/code&gt; cluster. This procedure is written assuming a &lt;code&gt;Consul&lt;/code&gt; server running on address &lt;code&gt;192.168.42.10:8500&lt;/code&gt;. All hosts will have a Docker Engine configured to listen on port 2375.  We will be configuring the Managers to operate on port 4000. The sample Swarm configuration has three machines:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;manager-1&lt;/code&gt; on &lt;code&gt;192.168.42.200&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;manager-2&lt;/code&gt; on &lt;code&gt;192.168.42.201&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;manager-3&lt;/code&gt; on &lt;code&gt;192.168.42.202&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;create-the-primary-manager&#34;&gt;Create the primary manager&lt;/h3&gt;

&lt;p&gt;You use the &lt;code&gt;swarm manage&lt;/code&gt; command with the &lt;code&gt;--replication&lt;/code&gt; and &lt;code&gt;--advertise&lt;/code&gt; flags to create a primary manager.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  user@manager-1 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.200:4000 consul://192.168.42.10:8500/nodes
  INFO[0000] Listening for HTTP addr=:4000 proto=tcp
  INFO[0000] Cluster leadership acquired
  INFO[0000] New leader elected: 192.168.42.200:4000
  [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The  &lt;code&gt;--replication&lt;/code&gt; flag tells Swarm that the manager is part of a multi-manager configuration and that this primary manager competes with other manager instances for the primary role. The primary manager has the authority to manage cluster, replicate logs, and replicate events happening inside the cluster.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;--advertise&lt;/code&gt; option specifies the primary manager address. Swarm uses this address to advertise to the cluster when the node is elected as the primary. As you see in the command&amp;rsquo;s output, the address you provided now appears to be the one of the elected Primary manager.&lt;/p&gt;

&lt;h3 id=&#34;create-two-replicas&#34;&gt;Create two replicas&lt;/h3&gt;

&lt;p&gt;Now that you have a primary manager, you can create replicas.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-2 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.201:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command creates a replica manager on &lt;code&gt;192.168.42.201:4000&lt;/code&gt; which is looking at &lt;code&gt;192.168.42.200:4000&lt;/code&gt; as the primary manager.&lt;/p&gt;

&lt;p&gt;Create an additional, third &lt;em&gt;manager&lt;/em&gt; instance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-3 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.202:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have established your primary manager and the replicas, create &lt;strong&gt;Swarm agents&lt;/strong&gt; as you normally would.&lt;/p&gt;

&lt;h3 id=&#34;list-machines-in-the-cluster&#34;&gt;List machines in the cluster&lt;/h3&gt;

&lt;p&gt;Typing &lt;code&gt;docker info&lt;/code&gt; should give you an output similar to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@my-machine $ export DOCKER_HOST=192.168.42.200:4000 # Points to manager-1
user@my-machine $ docker info
Containers: 0
Images: 25
Storage Driver:
Role: Primary  &amp;lt;--------- manager-1 is the Primary manager
Primary: 192.168.42.200
Strategy: spread
Filters: affinity, health, constraint, port, dependency
Nodes: 3
 swarm-agent-0: 192.168.42.100:2375
   Containers: 0
   Reserved CPUs: 0 / 1
   Reserved Memory: 0 B / 2.053 GiB
   Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
 swarm-agent-1: 192.168.42.101:2375
   Containers: 0
   Reserved CPUs: 0 / 1
   Reserved Memory: 0 B / 2.053 GiB
   Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
 swarm-agent-2: 192.168.42.102:2375
   Containers: 0
   Reserved CPUs: 0 / 1
   Reserved Memory: 0 B / 2.053 GiB
   Labels: executiondriver=native-0.2, kernelversion=3.13.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
Execution Driver:
Kernel Version:
Operating System:
CPUs: 3
Total Memory: 6.158 GiB
Name:
ID:
Http Proxy:
Https Proxy:
No Proxy:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This information shows that &lt;code&gt;manager-1&lt;/code&gt; is the current primary and supplies the address to use to contact this primary.&lt;/p&gt;

&lt;h2 id=&#34;test-the-failover-mechanism&#34;&gt;Test the failover mechanism&lt;/h2&gt;

&lt;p&gt;To test the failover mechanism, you shut down the designated primary manager.
Issue a &lt;code&gt;Ctrl-C&lt;/code&gt; or &lt;code&gt;kill&lt;/code&gt; the current primary manager (&lt;code&gt;manager-1&lt;/code&gt;) to shut it down.&lt;/p&gt;

&lt;h3 id=&#34;wait-for-automated-failover&#34;&gt;Wait for automated failover&lt;/h3&gt;

&lt;p&gt;After a short time, the other instances detect the failure and a replica takes the &lt;em&gt;lead&lt;/em&gt; to become the primary manager.&lt;/p&gt;

&lt;p&gt;For example, look at &lt;code&gt;manager-2&lt;/code&gt;&amp;rsquo;s logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-2 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.201:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
INFO[0038] New leader elected: 192.168.42.201:4000
INFO[0038] Cluster leadership acquired               &amp;lt;--- We have been elected as the new Primary Manager
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because the primary manager, &lt;code&gt;manager-1&lt;/code&gt;, failed right after it was elected, the replica with the address &lt;code&gt;192.168.42.201:4000&lt;/code&gt;, &lt;code&gt;manager-2&lt;/code&gt;, recognized the failure and attempted to take away the lead. Because &lt;code&gt;manager-2&lt;/code&gt; was fast enough, the process was effectively elected as the primary manager. As a result, &lt;code&gt;manager-2&lt;/code&gt; became the primary manager of the cluster.&lt;/p&gt;

&lt;p&gt;If we take a look at &lt;code&gt;manager-3&lt;/code&gt; we should see those &lt;code&gt;logs&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@manager-3 $ swarm manage -H :4000 &amp;lt;tls-config-flags&amp;gt; --replication --advertise 192.168.42.202:4000 consul://192.168.42.10:8500/nodes
INFO[0000] Listening for HTTP                            addr=:4000 proto=tcp
INFO[0000] Cluster leadership lost
INFO[0000] New leader elected: 192.168.42.200:4000
INFO[0036] New leader elected: 192.168.42.201:4000   &amp;lt;--- manager-2 sees the new Primary Manager
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point, we need to export the new &lt;code&gt;DOCKER_HOST&lt;/code&gt; value.&lt;/p&gt;

&lt;h3 id=&#34;switch-the-primary&#34;&gt;Switch the primary&lt;/h3&gt;

&lt;p&gt;To switch the &lt;code&gt;DOCKER_HOST&lt;/code&gt; to use &lt;code&gt;manager-2&lt;/code&gt; as the primary, you do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user@my-machine $ export DOCKER_HOST=192.168.42.201:4000 # Points to manager-2
user@my-machine $ docker info
Containers: 0
Images: 25
Storage Driver:
Role: Replica  &amp;lt;--------- manager-2 is a Replica
Primary: 192.168.42.200
Strategy: spread
Filters: affinity, health, constraint, port, dependency
Nodes: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use the &lt;code&gt;docker&lt;/code&gt; command on any Docker Swarm primary manager or any replica.&lt;/p&gt;

&lt;p&gt;If you like, you can use custom mechanisms to always point &lt;code&gt;DOCKER_HOST&lt;/code&gt; to the current primary manager. Then, you never lose contact with your Docker Swarm in the event of a failover.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to get Swarm</title>
      <link>http://docs-stage.docker.com/swarm/get-swarm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/get-swarm/</guid>
      <description>

&lt;h1 id=&#34;how-to-get-docker-swarm&#34;&gt;How to get Docker Swarm&lt;/h1&gt;

&lt;p&gt;You can create a Docker Swarm cluster using the &lt;code&gt;swarm&lt;/code&gt; executable image from a
container or using an executable &lt;code&gt;swarm&lt;/code&gt; binary you install on your system. This
page introduces the two methods and discusses their pros and cons.&lt;/p&gt;

&lt;h2 id=&#34;create-a-cluster-with-an-interactive-container&#34;&gt;Create a cluster with an interactive container&lt;/h2&gt;

&lt;p&gt;You can use the Docker Swarm official image to create a cluster. The image is
built by Docker and updated regularly through an automated build. To use the
image, you run it a container via the Engine &lt;code&gt;docker run&lt;/code&gt; command. The image has
multiple options and subcommands you can use to create and manage a Swarm cluster.&lt;/p&gt;

&lt;p&gt;The first time you use any image, Docker Engine checks to see if you already have the image in your environment. By default Docker runs the &lt;code&gt;swarm:latest&lt;/code&gt; version but you can also specify a tag other than &lt;code&gt;latest&lt;/code&gt;. If you have an image locally but a newer one exists on Docker Hub, Engine downloads it.&lt;/p&gt;

&lt;h3 id=&#34;run-the-swarm-image-from-a-container&#34;&gt;Run the Swarm image from a container&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open a terminal on a host running Engine.&lt;/p&gt;

&lt;p&gt;If you are using Mac or Windows, then you must make sure you have started a Docker Engine host running and pointed your terminal environment to it with the Docker Machine commands. If you aren&amp;rsquo;t sure, you can verify:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine ls
NAME      ACTIVE   URL          STATE     URL                         SWARM   DOCKER    ERRORS
default   *       virtualbox   Running   tcp://192.168.99.100:2376           v1.9.1    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This shows an environment running an Engine host on the &lt;code&gt;default&lt;/code&gt; instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;swarm&lt;/code&gt; image to execute a command.&lt;/p&gt;

&lt;p&gt;The easiest command is to get the help for the image. This command shows all the options that are available with the image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run swarm --help
Unable to find image &#39;swarm:latest&#39; locally
latest: Pulling from library/swarm
d681c900c6e3: Pull complete
188de6f24f3f: Pull complete
90b2ffb8d338: Pull complete
237af4efea94: Pull complete
3b3fc6f62107: Pull complete
7e6c9135b308: Pull complete
986340ab62f0: Pull complete
a9975e2cc0a3: Pull complete
Digest: sha256:c21fd414b0488637b1f05f13a59b032a3f9da5d818d31da1a4ca98a84c0c781b
Status: Downloaded newer image for swarm:latest
Usage: swarm [OPTIONS] COMMAND [arg...]

A Docker-native clustering system

Version: 1.0.1 (744e3a3)

Options:
  --debug           debug mode [$DEBUG]
  --log-level, -l &amp;quot;info&amp;quot;    Log level (options: debug, info, warn, error, fatal, panic)
  --help, -h            show help
  --version, -v         print the version

Commands:
  create, c Create a cluster
  list, l   List nodes in a cluster
  manage, m Manage a docker cluster
  join, j   join a docker cluster
  help, h   Shows a list of commands or help for one command

Run &#39;swarm COMMAND --help&#39; for more information on a command.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, the &lt;code&gt;swarm&lt;/code&gt; image did not exist on the Engine host, so the
Engine downloaded it. After it downloaded, the image executed the &lt;code&gt;help&lt;/code&gt;
subcommand to display the help text. After displaying the help, the &lt;code&gt;swarm&lt;/code&gt;
image exits and returns you to your terminal command line.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;List the running containers on your Engine host.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Swarm is no longer running. The &lt;code&gt;swarm&lt;/code&gt; image exits after you issue it a command.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;why-use-the-image&#34;&gt;Why use the image?&lt;/h3&gt;

&lt;p&gt;Using a Swarm container has three key benefits over other methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You don&amp;rsquo;t need to install a binary on the system to use the image.&lt;/li&gt;
&lt;li&gt;The single command &lt;code&gt;docker run&lt;/code&gt; command gets and run the most recent version of the image every time.&lt;/li&gt;
&lt;li&gt;The container isolates Swarm from your host environment. You don&amp;rsquo;t need to perform or maintain shell paths and environments.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Running the Swarm image is the recommended way to create and manage your Swarm cluster. All of Docker&amp;rsquo;s documentation and tutorials use this method.&lt;/p&gt;

&lt;h2 id=&#34;run-a-swarm-binary&#34;&gt;Run a Swarm binary&lt;/h2&gt;

&lt;p&gt;Before you run a Swarm binary directly on a host operating system (OS), you compile the binary from the source code or get a trusted copy from another location. Then you run the Swarm binary.&lt;/p&gt;

&lt;p&gt;To compile Swarm from source code, refer to the instructions in
&lt;a href=&#34;http://github.com/docker/swarm/blob/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;why-use-the-binary&#34;&gt;Why use the binary?&lt;/h3&gt;

&lt;p&gt;Using a Swarm binary this way has one key benefit over other methods: If you are
a developer who contributes to the Swarm project, you can test your code changes
without &amp;ldquo;containerizing&amp;rdquo; the binary before you run it.&lt;/p&gt;

&lt;p&gt;Running a Swarm binary on the host OS has disadvantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compilation from source is a burden.&lt;/li&gt;
&lt;li&gt;The binary doesn&amp;rsquo;t have the benefits that
Docker containers provide, such as isolation.&lt;/li&gt;
&lt;li&gt;Most Docker documentation and tutorials don&amp;rsquo;t show this method of running swarm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lastly, because the Swarm nodes don&amp;rsquo;t use Engine, you can&amp;rsquo;t use Docker-based
software tools, such as Docker Engine CLI at the node level.&lt;/p&gt;

&lt;h2 id=&#34;related-information&#34;&gt;Related information&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/_/swarm/&#34;&gt;Docker Swarm official image&lt;/a&gt; repository on Docker Hub&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/provision-with-machine/&#34;&gt;Provision a Swarm with Docker Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learn the application architecture</title>
      <link>http://docs-stage.docker.com/swarm/swarm_at_scale/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/swarm_at_scale/about/</guid>
      <description>

&lt;h1 id=&#34;learn-the-application-architecture&#34;&gt;Learn the application architecture&lt;/h1&gt;

&lt;p&gt;On this page, you learn about the Swarm at scale example.  Make sure you have
read through &lt;a href=&#34;../swarm/swarm_at_scale/&#34;&gt;the introduction&lt;/a&gt; to get an idea of the skills and time
required first.&lt;/p&gt;

&lt;h2 id=&#34;learn-the-example-back-story&#34;&gt;Learn the example back story&lt;/h2&gt;

&lt;p&gt;Your company is a pet food company that has bought a commercial during the
Superbowl. The commercial drives viewers to a web survey that asks users to vote
&amp;ndash; cats or dogs. You are developing the web survey.&lt;/p&gt;

&lt;p&gt;Your survey must ensure that millions of people can vote concurrently without
your website becoming unavailable. You don&amp;rsquo;t need real-time results, a company
press release announces the results. However, you do need confidence that every
vote is counted.&lt;/p&gt;

&lt;h2 id=&#34;understand-the-application-architecture&#34;&gt;Understand the application architecture&lt;/h2&gt;

&lt;p&gt;The voting application is composed of several microservices. It uses a parallel
web frontend that sends jobs to asynchronous background workers. The
application&amp;rsquo;s design can accommodate arbitrarily large scale. The diagram below
shows the appliation&amp;rsquo;s high level architecture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/app-architecture.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;All the servers are running Docker Engine. The entire application is fully
&amp;ldquo;Dockerized&amp;rdquo; in that all services are running inside of containers.&lt;/p&gt;

&lt;p&gt;The frontend consists of a load balancer with &lt;em&gt;N&lt;/em&gt; frontend instances. Each
frontend consists of a web server and a Redis queue. The load balancer can
handle an arbitrary number of web containers behind it (&lt;code&gt;frontend01&lt;/code&gt;-
&lt;code&gt;frontendN&lt;/code&gt;). The web containers run a simple Python application that takes a
vote between two options. It queues the votes to a Redis container running on
the datastore.&lt;/p&gt;

&lt;p&gt;Behind the frontend is a worker tier which runs on separate nodes. This tier:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;scans the Redis containers&lt;/li&gt;
&lt;li&gt;dequeues votes&lt;/li&gt;
&lt;li&gt;deduplicates votes to prevent double voting&lt;/li&gt;
&lt;li&gt;commits the results to a Postgres database&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Just like the frontend, the worker tier can also scale arbitrarily. The worker
count and frontend count are independent from each other.&lt;/p&gt;

&lt;p&gt;The application&amp;rsquo;s Dockerized microservices are deployed to a container network.
Container networks are a feature of Docker Engine that allows communication
between multiple containers across multiple Docker hosts.&lt;/p&gt;

&lt;h2 id=&#34;swarm-cluster-architecture&#34;&gt;Swarm Cluster Architecture&lt;/h2&gt;

&lt;p&gt;To support the application, the design calls for a Swarm cluster with a single
Swarm manager and four nodes as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/swarm-cluster-arch.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;All four nodes in the cluster are running the Docker daemon, as is the Swarm
manager and the load balancer. The Swarm manager is part of the cluster and is
considered out of band for the application. A single host running the Consul
server acts as a keystore for both Swarm discovery and for the container
network. The load balancer could be placed inside of the cluster, but for this
demonstration it is not.&lt;/p&gt;

&lt;p&gt;After completing the example and deploying your application, this
is what your environment should look like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/final-result.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As the previous diagram shows, each node in the cluster runs the following containers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;frontend01&lt;/code&gt;:

&lt;ul&gt;
&lt;li&gt;Container: voting-app&lt;/li&gt;
&lt;li&gt;Container: Swarm agent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;frontend02&lt;/code&gt;:

&lt;ul&gt;
&lt;li&gt;Container: voting-app&lt;/li&gt;
&lt;li&gt;Container: Swarm agent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worker01&lt;/code&gt;:

&lt;ul&gt;
&lt;li&gt;Container: voting-app-worker&lt;/li&gt;
&lt;li&gt;Container: Swarm agent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dbstore&lt;/code&gt;:

&lt;ul&gt;
&lt;li&gt;Container: voting-app-result-app&lt;/li&gt;
&lt;li&gt;Container: db (Postgres 9.4)&lt;/li&gt;
&lt;li&gt;Container: redis&lt;/li&gt;
&lt;li&gt;Container: Swarm agent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After deploying the application, you&amp;rsquo;ll configure your local system so that you
can test the application from your local browser. In production, of course, this
step wouldn&amp;rsquo;t be needed.&lt;/p&gt;

&lt;h2 id=&#34;next-step&#34;&gt;Next step&lt;/h2&gt;

&lt;p&gt;Now that you understand the application architecture, you need to deploy a
network configuration that can support it. In the next step, you
&lt;a href=&#34;../swarm/swarm_at_scale/deploy-infra/&#34;&gt;deploy network infrastructure&lt;/a&gt; for use in this sample.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Overview Docker Swarm with TLS</title>
      <link>http://docs-stage.docker.com/swarm/secure-swarm-tls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://docs-stage.docker.com/swarm/secure-swarm-tls/</guid>
      <description>

&lt;h1 id=&#34;overview-swarm-with-tls&#34;&gt;Overview Swarm with TLS&lt;/h1&gt;

&lt;p&gt;All nodes in a Swarm cluster must bind their Docker daemons to a network port.
This has obvious security implications. These implications are compounded when
the network in question is untrusted such as the internet. To mitigate these
risks, Docker Swarm and the Docker Engine daemon support Transport Layer Security
(TLS).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: TLS is the successor to SSL (Secure Sockets Layer) and the two
terms are often used interchangeably. Docker uses TLS, this
term is used throughout this article.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;learn-the-tls-concepts&#34;&gt;Learn the TLS concepts&lt;/h2&gt;

&lt;p&gt;Before going further, it is important to understand the basic concepts of TLS
and public key infrastructure (PKI).&lt;/p&gt;

&lt;p&gt;Public key infrastructure is a combination of security related technologies,
policies, and procedures, that are used to create and manage digital
certificates. These certificates and infrastructure secure digital
communication using mechanisms such as authentication and encryption.&lt;/p&gt;

&lt;p&gt;The following analogy may be useful. It is common practice that passports are
used to verify an individual&amp;rsquo;s identity. Passports usually contain a photograph
and biometric information that identify the owner. A passport also lists the
country that issued it, as well as &lt;em&gt;valid from&lt;/em&gt; and &lt;em&gt;valid to&lt;/em&gt; dates. Digital
certificates are very similar. The text below is an extract from a digital
certificate:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Certificate:
Data:
    Version: 3 (0x2)
    Serial Number: 9590646456311914051 (0x8518d2237ad49e43)
Signature Algorithm: sha256WithRSAEncryption
    Issuer: C=US, ST=CA, L=Sanfrancisco, O=Docker Inc
    Validity
        Not Before: Jan 18 09:42:16 2016 GMT
        Not After : Jan 15 09:42:16 2026 GMT
    Subject: CN=swarm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This certificate identifies a computer called &lt;strong&gt;swarm&lt;/strong&gt;. The certificate is valid between January 2016 and January 2026 and was issued by Docker Inc. based in the state of California in the US.&lt;/p&gt;

&lt;p&gt;Just as passports authenticate individuals as they board flights and clear
customs, digital certificates authenticate computers on a network.&lt;/p&gt;

&lt;p&gt;Public key infrastructure (PKI) is the combination of technologies, policies,
and procedures that work behind the scenes to enable digital certificates. Some
of the technologies, policies and procedures provided by PKI include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Services to securely request certificates&lt;/li&gt;
&lt;li&gt;Procedures to authenticate the entity requesting the certificate&lt;/li&gt;
&lt;li&gt;Procedures to determine the entity&amp;rsquo;s eligibility for the certificate&lt;/li&gt;
&lt;li&gt;Technologies and processes to issue certificates&lt;/li&gt;
&lt;li&gt;Technologies and processes to revoke certificates&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-does-docker-engine-authenticate-using-tls&#34;&gt;How does Docker Engine authenticate using TLS&lt;/h2&gt;

&lt;p&gt;In this section, you&amp;rsquo;ll learn how Docker Engine and Swarm use PKI and
certificates to increase security.&lt;/p&gt;

&lt;!--[metadata]&gt;Need to know about encryption too&lt;![end-metadata]--&gt;

&lt;p&gt;You can configure both the Docker Engine CLI and the Docker Engine daemon to require
TLS for authentication. Configuring TLS means that all communications between
the Docker Engine CLI and the Docker Engine daemon must be accompanied with, and signed by a
trusted digital certificate. The Docker Engine CLI must provide its digital certificate
before the Docker Engine daemon will accept incoming commands from it.&lt;/p&gt;

&lt;p&gt;The Docker Engine daemon must also trust the certificate that the Docker Engine CLI uses.
This trust is usually established by way of a trusted third party. The Docker Engine
CLI and Docker Engine daemon in the diagram below are configured to require TLS
authentication.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../swarm/images/trust-diagram.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The trusted third party in this diagram is the the Certificate Authority (CA)
server. Like the country in the passport example, a CA creates, signs, issues,
revokes certificates. Trust is established by installing the CA&amp;rsquo;s root
certificate on the host running the Docker Engine daemon. The Docker Engine CLI then requests
its own certificate from the CA server, which the CA server signs and issues to
the client.&lt;/p&gt;

&lt;p&gt;The Docker Engine CLI sends its certificate to the Docker Engine daemon before issuing
commands. The Docker Engine daemon inspects the certificate, and because the Docker Engine daemon trusts the CA,
the Docker Engine daemon automatically trusts any certificates signed by the CA. Assuming the
certificate is in order (the certificate has not expired or been revoked etc.)
the Docker Engine daemon accepts commands from this trusted Docker Engine CLI.&lt;/p&gt;

&lt;p&gt;The Docker Engine CLI is simply a client that uses the Docker Engine Remote API to
communicate with the Docker Engine daemon. Any client that uses this Docker Engine Remote API can use
TLS. For example, Dcoker Engine clients such as &amp;lsquo;Docker Universal Control Plane&amp;rsquo;
(UCP) have TLS support built-in. Other, third party products, that use Docker Engine
Remote API, can also be configured this way.&lt;/p&gt;

&lt;h2 id=&#34;tls-modes-with-docker-and-swarm&#34;&gt;TLS modes with Docker and Swarm&lt;/h2&gt;

&lt;p&gt;Now that you know how certificates are used by the Docker Engine daemon for authentication,
it&amp;rsquo;s important to be aware of the three TLS configurations possible with Docker
Engine daemon and its clients:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;External 3rd party CA&lt;/li&gt;
&lt;li&gt;Internal corporate CA&lt;/li&gt;
&lt;li&gt;Self-signed certificates&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These configurations are differentiated by the type of entity acting as the Certificate Authority (CA).&lt;/p&gt;

&lt;h3 id=&#34;external-3rd-party-ca&#34;&gt;External 3rd party CA&lt;/h3&gt;

&lt;p&gt;An external CA is a trusted 3rd party company that provides a means of creating,
issuing, revoking, and otherwise managing certificates. They are &lt;em&gt;trusted&lt;/em&gt; in
the sense that they have to fulfill specific conditions and maintain high levels
of security and business practices to win your business. You also have to
install the external CA&amp;rsquo;s root certificates for you computers and services to
&lt;em&gt;trust&lt;/em&gt; them.&lt;/p&gt;

&lt;p&gt;When you use an external 3rd party CA, they create, sign, issue, revoke and
otherwise manage your certificates. They normally charge a fee for these
services, but are considered an enterprise-class scalable solution that
provides a high degree of trust.&lt;/p&gt;

&lt;h3 id=&#34;internal-corporate-ca&#34;&gt;Internal corporate CA&lt;/h3&gt;

&lt;p&gt;Many organizations choose to implement their own Certificate Authorities and
PKI. Common examples are using OpenSSL and Microsoft Active Directory. In this
case, your company is its own Certificate Authority with all the work it
entails. The benefit is, as your own CA, you have more control over your PKI.&lt;/p&gt;

&lt;p&gt;Running your own CA and PKI requires you to provide all of the services offered
by external 3rd party CAs. These include creating, issuing, revoking, and
otherwise managing certificates. Doing all of this yourself has its own costs
and overheads. However, for a large corporation, it still may reduce costs in
comparison to using an external 3rd party service.&lt;/p&gt;

&lt;p&gt;Assuming you operate and manage your own internal CAs and PKI properly, an
internal, corporate CA  can be a highly scalable and highly secure option.&lt;/p&gt;

&lt;h3 id=&#34;self-signed-certificates&#34;&gt;Self-signed certificates&lt;/h3&gt;

&lt;p&gt;As the name suggests, self-signed certificates are certificates that are signed
with their own private key rather than a trusted CA. This is a low cost and
simple to use option. If you implement and manage self-signed certificates
correctly, they can be better than using no certificates.&lt;/p&gt;

&lt;p&gt;Because self-signed certificates lack of a full-blown PKI, they do not scale
well and lack many of the advantages offered by the other options. One of their
disadvantages is that you cannot revoke self-signed certificates. Due to this, and
other limitations, self-signed certificates are considered the least secure of
the three options. Self-signed certificates are not recommended for public
facing production workloads exposed to untrusted networks.&lt;/p&gt;

&lt;h2 id=&#34;related-information&#34;&gt;Related information&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../swarm/configure-tls/&#34;&gt;Configure Docker Swarm for TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/security/security/&#34;&gt;Docker security&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>