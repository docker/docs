---
title: "Docker Reference Architecture: Docker Logging Design and Best Practices"
id: 2238
draftstate: inactive
deleted: false
layout: docs
permalink: /kb/2238/
source: https://success.docker.com/@api/deki/pages/2238/contents
tags:
- tag: "article:reference"
- tag: "product:datacenter"
- tag: "stage:reviewed"
- tag: "testedon:docker-17.03.0-ee-5"
- tag: "testedon:docker-17.06.1-ee"
---
{% raw %}
<script type="text/javascript">
 /*<![CDATA[*/
var authorByline = "Andrew Hromis";
/*]]>*/
</script>
<div class="mt-section" id="section_1" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Introduction">
 </span>
 <h2 id="1-0">
  Introduction
 </h2>
 <p>
  Traditionally, designing and implementing centralized logging is an after-thought. It is not until problems arise that priorities shift to a centralized logging solution to query, view, and analyze the logs so the root-cause of the problem can be found. However, in the container era, when designing a Containers-as-a-Service (CaaS) platform with Docker Enterprise Edition (Docker EE), it is critical to prioritize centralized logging. As the number of micro-services deployed in containers increases, the amount of data produced by them in the form of logs (or events) exponentially increases.
 </p>
</div>
<div class="mt-section" id="section_2" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="What_You_Will_Learn">
 </span>
 <h2 id="1-1">
  What You Will Learn
 </h2>
 <p>
  This reference architecture provides an overview of how Docker logging works, explains the two main categories of Docker logs, and then discusses Docker logging best practices.
 </p>
</div>
<div class="mt-section" id="section_3" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Understanding_Docker_Logging">
 </span>
 <h2 id="1-2">
  Understanding Docker Logging
 </h2>
 <p>
  Before diving into design considerations, it's important to start with the basics of Docker logging.
 </p>
 <p>
  Docker supports different logging drivers used to store and/or stream
  <strong>
   container
  </strong>
  <code>
   stdout
  </code>
  and
  <code>
   stderr
  </code>
  logs of the main container process (
  <code>
   pid 1
  </code>
  ). By default, Docker uses the
  <code>
   json-file
  </code>
  logging driver, but it can be configured to use
  <a class="link-https" href="https://docs.docker.com/engine/admin/logging/overview/#supported-logging-drivers" rel="external nofollow" target="_blank">
   many other drivers
  </a>
  by setting the value of
  <code>
   log-driver
  </code>
  in
  <code>
   /etc/docker/daemon.json
  </code>
  followed by restarting the Docker daemon to reload its configuration.
 </p>
 <p>
  The logging driver settings apply to
  <strong>
   ALL
  </strong>
  containers launched after reconfiguring the daemon (restarting existing containers after reconfiguring the logging driver does not result in containers using the updated config). To override the default container logging driver run the container with
  <code>
   --log-driver
  </code>
  and
  <code>
   --log-opt
  </code>
  options. Swarm-mode services, on the other hand, can be updated to use a different logging driver on the go by using
  <code>
   docker service update --log-driver &lt;DRIVER_NAME&gt; --log-opt &lt;LIST OF OPTIONS&gt; &lt;SERVICE NAME&gt;
  </code>
  .
 </p>
 <p>
  What about Docker Engine logs? These logs are typically handled by the default system manager logger. Most of the modern distros (CentOS 7, RHEL 7, Ubuntu 16, etc.) use
  <code>
   systemd
  </code>
  , which uses
  <code>
   journald
  </code>
  for logging and
  <code>
   journalctl
  </code>
  for accessing the logs. To access the Engine logs use
  <code>
   journalctl -u docker.service
  </code>
  .
 </p>
</div>
<div class="mt-section" id="section_4" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Docker_Logs_Categories_and_Sources">
 </span>
 <h2 id="1-3">
  Docker Logs Categories and Sources
 </h2>
 <p>
  Now that the basics of Docker logging have been covered, this section explains their
  <strong>
   categories
  </strong>
  and
  <strong>
   sources
  </strong>
  .
 </p>
 <p>
  Docker logs typically fall into one of two categories:
  <strong>
   Infrastructure Management
  </strong>
  or
  <strong>
   Application
  </strong>
  logs. Most logs naturally fall into these categories based on the roles of who needs access to the logs.
 </p>
 <ul>
  <li>
   <p>
    Operators are mostly concerned with the stability of the platform as well as the availability of the services.
   </p>
  </li>
  <li>
   <p>
    Developers are more concerned with their application code and how their service is performing.
   </p>
  </li>
 </ul>
 <p>
  In order to have a self-service platform, both operators and developers should have access to the logs they need in order to perform their role. DevOps practices suggest that there is an overall, shared responsibility when it comes to service availability and performance. However, everyone shouldn't need access to every log on the platform. For instance, developers should only need access to the logs for their services and the integration points. Operators are more concerned with Docker daemon logs, UCP and DTR availability, as well as service availability. There is a bit of overlap since developers and operators both should be aware of service availability. Having access to the logs that each role needs allows for simpler troubleshooting when an issues occurs and a decreased Mean Time To Resolve (MTTR).
 </p>
 <div class="mt-section" id="section_5" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
  <span id="Infrastructure_Management_Logs">
  </span>
  <h3 id="2-0">
   Infrastructure Management Logs
  </h3>
  <p>
   The infrastructure management logs include the logs of the Docker Engine, containers running UCP or DTR, and any containerized infrastructure services that are deployed (think containerized monitoring agents).
  </p>
  <div class="mt-section" id="section_6" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
   <span id="Docker_Engine_Logs">
   </span>
   <h4 id="3-0">
    Docker Engine Logs
   </h4>
   <p>
    As previously mentioned, Docker Engine logs are captured by the OS's system manager by default. These logs can be sent to a centralized logging server.
   </p>
  </div>
  <div class="mt-section" id="section_7" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
   <span id="UCP_and_DTR_System_Logs">
   </span>
   <h4 id="3-1">
    UCP and DTR System Logs
   </h4>
   <p>
    UCP and DTR are deployed as Docker containers. All their logs are captured in the containers'
    <code>
     STDOUT
    </code>
    /
    <code>
     STDERR
    </code>
    . The default logging driver for Docker Engine captures these logs.
   </p>
   <p>
    UCP can be configured to use remote syslog logging. This can be done post-installation from the UCP UI for all of its containers.
   </p>
   <blockquote>
    <p>
     Note: It is recommended that the Docker Engine default logging driver be configured
     <strong>
      before
     </strong>
     installing UCP and DTR so that their logs are captured by the chosen logging driver. This is due to the inability to change a container's logging driver once it had been created. The only exception to this is
     <code>
      ucp-agent
     </code>
     , which is a component of UCP that gets deployed as a Swarm service.
    </p>
   </blockquote>
  </div>
  <div class="mt-section" id="section_8" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
   <span id="Infrastructure_Services">
   </span>
   <h4 id="3-2">
    Infrastructure Services
   </h4>
   <p>
    Infrastructure operation teams deploy containerized infrastructure services used for various infrastructure operations such as monitoring, auditing, reporting, config deployment, etc. These services also produce important logs that need to be captured. Typically, their logs are limited to the
    <code>
     STDOUT
    </code>
    /
    <code>
     STDERR
    </code>
    of their containers, so they are also captured by the Docker Engine default logging driver. If not, they need to be handled separately.
   </p>
  </div>
 </div>
 <div class="mt-section" id="section_9" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
  <span id="Application_Logs">
  </span>
  <h3 id="2-1">
   Application Logs
  </h3>
  <p>
   Application-produced logs can be a combination of custom application logs and the
   <code>
    STDOUT
   </code>
   /
   <code>
    STDERR
   </code>
   logs of the main process of the application. As described earlier, the
   <code>
    STDOUT
   </code>
   /
   <code>
    STDERR
   </code>
   logs of all containers are captured by the Docker Engine default logging driver. So, no need to do any custom configuration to capture them. If the application has custom logging ( e.g. writes logs to
   <code>
    /var/log/myapp.log
   </code>
   within the container), it's important to take that into consideration.
  </p>
 </div>
</div>
<div class="mt-section" id="section_10" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Docker_Logging_Design_Considerations">
 </span>
 <h2 id="1-4">
  Docker Logging Design Considerations
 </h2>
 <p>
  Understanding the types of Docker logs is important. It is also important to define which entities are best suited to consume and own them.
 </p>
 <div class="mt-section" id="section_11" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
  <span id="Categorizing_the_Docker_Logs">
  </span>
  <h3 id="2-2">
   Categorizing the Docker Logs
  </h3>
  <p>
   Mainly, there are two categories: infrastructure logs and application logs.
  </p>
 </div>
 <div class="mt-section" id="section_12" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
  <span id="Defining_the_Organizational_Ownership">
  </span>
  <h3 id="2-3">
   Defining the Organizational Ownership
  </h3>
  <p>
   Based on the organization's structure and policies, decide if these categories have a direct mapping to existing teams. If they do not, then it is important to define the right organization or team responsible for these log categories:
  </p>
  <table>
   <thead>
    <tr>
     <th>
      Category
     </th>
     <th>
      Team
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      System and Management Logs
     </td>
     <td>
      Infrastructure Operations
     </td>
    </tr>
    <tr>
     <td>
      Application Logs
     </td>
     <td>
      Application Operations
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   If the organization is part of a larger organization, these categories may be too broad. Sub-divide them into more specific ownership teams:
  </p>
  <table>
   <thead>
    <tr>
     <th>
      Category
     </th>
     <th>
      Team
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      Docker Engine Logs
     </td>
     <td>
      Infrastructure Operations
     </td>
    </tr>
    <tr>
     <td>
      Infrastructure Services
     </td>
     <td>
      Infrastructure Operations
     </td>
    </tr>
    <tr>
     <td>
      UCP and DTR Logs
     </td>
     <td>
      UCP/DTR Operations
     </td>
    </tr>
    <tr>
     <td>
      Application A Logs
     </td>
     <td>
      Application A Operations
     </td>
    </tr>
    <tr>
     <td>
      Application B Logs
     </td>
     <td>
      Application B Operations
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   Some organizations don't distinguish between infrastructure and application operations, so they might combine the two categories and have a single operations team own them.
  </p>
  <table>
   <thead>
    <tr>
     <th>
      Category
     </th>
     <th>
      Team
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      System and Management Logs
     </td>
     <td>
      Infrastructure Operations
     </td>
    </tr>
    <tr>
     <td>
      Application Logs
     </td>
     <td>
      Infrastructure Operations
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   Pick the right model to clearly define the appropriate ownership for each type of log, resulting in decreased mean time to resolve (MTTR). Once organizational ownership has been determined for the type of logs, it is time to start investigating the right logging solution for deployment.
  </p>
 </div>
 <div class="mt-section" id="section_13" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
  <span id="Picking_a_Logging_Infrastructure">
  </span>
  <h3 id="2-4">
   Picking a Logging Infrastructure
  </h3>
  <p>
   Docker can easily integrate with existing logging tools and solutions. Most of the major logging utilities in the logging ecosystem have developed Docker logging or provided proper documentation to integrate with Docker.
  </p>
  <p>
   Pick the logging solution that:
  </p>
  <ol>
   <li>
    Allows for the implementation of the organizational ownership model defined in the previous section. For example, some organizations may choose to send all logs to a single logging infrastructure and then provide the right level of access to the functional teams.
   </li>
   <li>
    The organization is most familiar with! This is a must!
   </li>
   <li>
    Has Docker integration: pre-configured dashboards, stable Docker plugin, proper documentation, etc.
   </li>
  </ol>
 </div>
</div>
<div class="mt-section" id="section_14" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Docker_Enterprise_Edition_Logs">
 </span>
 <h2 id="1-5">
  Docker Enterprise Edition Logs
 </h2>
 <p>
  With Docker Enterprise Edition, it's a good idea to store all of the container logs for historical and system maintenance purposes. It is recommended that you collect the output of some of the containers in an indexable way, mostly for policy reasons and to quickly understand cluster events. In the following sections we'll break down some Docker EE components and certain logs that could be useful to an organization.
 </p>
 <div class="mt-section" id="section_15" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
  <span id="UCP">
  </span>
  <h3 id="2-5">
   UCP
  </h3>
  <table>
   <thead>
    <tr>
     <th>
      Container Name
     </th>
     <th>
      Information in Logs
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      ucp-controller
     </td>
     <td>
      UCP API logs, users, and logins
     </td>
    </tr>
    <tr>
     <td>
      ucp-auth-api
     </td>
     <td>
      Centralized service for identity and authentication used by UCP and DTR
     </td>
    </tr>
    <tr>
     <td>
      ucp-auth-store
     </td>
     <td>
      Stores authentication configurations as well as data for users, organizations, and teams
     </td>
    </tr>
    <tr>
     <td>
      ucp-auth-worker
     </td>
     <td>
      Performs scheduled LDAP synchronizations and cleans authentication and authorization data
     </td>
    </tr>
    <tr>
     <td>
      ucp-client-root-ca
     </td>
     <td>
      Certificate authority to sign client bundles
     </td>
    </tr>
    <tr>
     <td>
      ucp-cluster-root-ca
     </td>
     <td>
      Certificate authority to sign client bundles
     </td>
    </tr>
    <tr>
     <td>
      ucp-metrics
     </td>
     <td>
      Used for metrics gathering
     </td>
    </tr>
    <tr>
     <td>
      ucp-kv
     </td>
     <td>
      etcd service used to store the UCP configurations
     </td>
    </tr>
    <tr>
     <td>
      ucp-proxy
     </td>
     <td>
      TLS proxy that allows secure access from the local Docker Engine to UCP components
     </td>
    </tr>
    <tr>
     <td>
      ucp-swarm-manager
     </td>
     <td>
      Manager container for Docker Swarm (classic), provides backwards compatibility
     </td>
    </tr>
   </tbody>
  </table>
  <ol>
   <li>
    <strong>
     ucp-controller
    </strong>
    - This container logs all login attempts and general usage of the cluster. For auditing purposes, this contains the most important logs in terms of logging usage of the cluster.
   </li>
   <li>
    <strong>
     ucp-kv
    </strong>
    - This container is good to monitor to make sure quorum is not lost on the cluster. If the quorum is lost, it is good practice to setup an alert.
   </li>
  </ol>
 </div>
 <div class="mt-section" id="section_16" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
  <span id="DTR">
  </span>
  <h3 id="2-6">
   DTR
  </h3>
  <table>
   <thead>
    <tr>
     <th>
      Name
     </th>
     <th>
      Information in Logs
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      dtr-api-&lt;replica_id&gt;
     </td>
     <td>
      Executes the DTR business logic and serves the DTR web application and API
     </td>
    </tr>
    <tr>
     <td>
      dtr-garant-&lt;replica_id&gt;
     </td>
     <td>
      Manages DTR authentication
     </td>
    </tr>
    <tr>
     <td>
      dtr-jobrunner-&lt;replica_id&gt;
     </td>
     <td>
      Runs cleanup jobs in the background
     </td>
    </tr>
    <tr>
     <td>
      dtr-nautilusstore-&lt;replica_id&gt;
     </td>
     <td>
      Stores security scanning data
     </td>
    </tr>
    <tr>
     <td>
      dtr-nginx-&lt;replica_id&gt;
     </td>
     <td>
      Receives HTTP and HTTPS requests and proxies them to other DTR components, by default listens to ports 80 and 443 of the host
     </td>
    </tr>
    <tr>
     <td>
      dtr-notary-server-&lt;replica_id&gt;
     </td>
     <td>
      Receives, validates, and serves content trust metadata, and is consulted when pushing or pulling to DTR with Content Trust enabled
     </td>
    </tr>
    <tr>
     <td>
      dtr-notary-signer-&lt;replica_id&gt;
     </td>
     <td>
      Performs server-side timestamp and snapshot signing for content trust metadata
     </td>
    </tr>
    <tr>
     <td>
      dtr-registry-&lt;replica_id&gt;
     </td>
     <td>
      Implements the functionality for pulling and pushing Docker images, also handles how images are stored
     </td>
    </tr>
    <tr>
     <td>
      dtr-rethinkdb-&lt;replica_id&gt;
     </td>
     <td>
      Stores persisting repository metadata in a database
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   Some notable logs to possibly setup regular expressions on:
  </p>
  <ol>
   <li>
    <strong>
     dtr-registry-&lt;replica_id&gt;
    </strong>
    - Parsing this container will show the client IP's, the user, and general usage of the cluster.
   </li>
   <li>
    <strong>
     dtr-nginx-&lt;replica_id&gt;
    </strong>
    - This container logs all pushes, pulls, and API calls to the cluster.
   </li>
   <li>
    <strong>
     dtr-rethinkdb-&lt;replica_id&gt;
    </strong>
    - The logs in this container contain information about the quorum state of RethinkDB. This is good to monitor and be alerted on any loss of quorum.
   </li>
  </ol>
  <div class="mt-section" id="section_17" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
   <span id="HTTP_Routing_Mesh">
   </span>
   <h4 id="3-3">
    HTTP Routing Mesh
   </h4>
   <p>
    By default the HTTP Routing Mesh doesn't log any requests to
    <code>
     STDOUT
    </code>
    . To log from HRM run the following command:
   </p>
   <pre>
<code>docker service update --env-add DEBUG=1 ucp-hrm
</code></pre>
   <p>
    Logs will then output from the HTTP Routing Mesh. If a log driver is configured at the Engine level, the log output follows the configuration at the Engine level.
   </p>
  </div>
 </div>
</div>
<div class="mt-section" id="section_18" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Application_Log_Drivers">
 </span>
 <h2 id="1-6">
  Application Log Drivers
 </h2>
 <p>
  Docker has several available logging drivers that can be used for the management of application logs. Check the
  <a class="link-https" href="https://docs.docker.com/engine/admin/logging/overview/#supported-logging-drivers" rel="external nofollow" target="_blank">
   Docker docs
  </a>
  for the complete list as well as detailed information on how to use them.
 </p>
 <p>
  As a general rule, if you already have logging infrastructure in place, then you should use the logging driver for that existing infrastructure. If you don't have an existing logging system in place, there's a few areas worth highlighting advantages and disadvantages.
 </p>
 <table>
  <thead>
   <tr>
    <th>
     Driver
    </th>
    <th>
     Advantages
    </th>
    <th>
     Disadvantages
    </th>
   </tr>
  </thead>
  <tbody>
   <tr>
    <td>
     none
    </td>
    <td>
     Ultra-secure, since nothing gets logged
    </td>
    <td>
     Much harder to troubleshoot issues with no logs
    </td>
   </tr>
   <tr>
    <td>
     <a class="link-https" href="https://docs.docker.com/engine/admin/logging/json-file/" rel="external nofollow" target="_blank">
      json-file
     </a>
    </td>
    <td>
     The default,
     <code>
      docker logs
     </code>
     works with it, supports tags
    </td>
    <td>
     Logs reside locally and not aggregated, logs can fill up local disk if no restrictions in place
    </td>
   </tr>
   <tr>
    <td>
     <a class="link-https" href="https://docs.docker.com/engine/admin/logging/syslog/" rel="external nofollow" target="_blank">
      syslog
     </a>
    </td>
    <td>
     Most machines come with syslog, only driver that supports TLS for encrypted log shipping, supports tags
    </td>
    <td>
     Needs to be set up as highly available (HA) or else there can be issues on container start if it's not available
    </td>
   </tr>
   <tr>
    <td>
     <a class="link-https" href="https://docs.docker.com/engine/admin/logging/journald/" rel="external nofollow" target="_blank">
      journald
     </a>
    </td>
    <td>
     <code>
      docker logs
     </code>
     also works with this driver, since it logs locally - log aggregator can be down without impact, this also collects Docker daemon logs
    </td>
    <td>
     Since journal logs are in binary format, extra steps need to be taken to ship them off to the log collector, no tag support
    </td>
   </tr>
   <tr>
    <td>
     <a class="link-https" href="https://docs.docker.com/engine/admin/logging/gelf/" rel="external nofollow" target="_blank">
      gelf
     </a>
    </td>
    <td>
     Provides indexable fields by defaults (container id, host, container name, etc.), tag support
    </td>
    <td>
     Only supports the UDP protocol, no
     <code>
      docker logs
     </code>
     support
    </td>
   </tr>
   <tr>
    <td>
     <a class="link-https" href="https://docs.docker.com/engine/admin/logging/fluentd/" rel="external nofollow" target="_blank">
      fluentd
     </a>
    </td>
    <td>
     Provides
     <code>
      container_name
     </code>
     and
     <code>
      container_id
     </code>
     fields by default, fluentd supports multiple outputs,
    </td>
    <td>
     No
     <code>
      docker logs
     </code>
     support for local logs
    </td>
   </tr>
   <tr>
    <td>
     <a class="link-https" href="https://docs.docker.com/engine/admin/logging/awslogs/" rel="external nofollow" target="_blank">
      awslogs
     </a>
    </td>
    <td>
     Easy integration when using Amazon Web Services, less infrastructure to maintain, tag support
    </td>
    <td>
     Not the most ideal for hybrid cloud configurations or on-premise installations, no
     <code>
      docker logs
     </code>
     support
    </td>
   </tr>
   <tr>
    <td>
     <a class="link-https" href="https://docs.docker.com/engine/admin/logging/splunk/" rel="external nofollow" target="_blank">
      splunk
     </a>
    </td>
    <td>
     Easy integration with Splunk, TLS support, highly configurable, tag support, additional metrics
    </td>
    <td>
     Splunk needs to be highly available or possible issues on container start, no
     <code>
      docker logs
     </code>
     support
    </td>
   </tr>
   <tr>
    <td>
     <a class="link-https" href="https://docs.docker.com/engine/admin/logging/etwlogs/" rel="external nofollow" target="_blank">
      etwlogs
     </a>
    </td>
    <td>
     Common framework for logging on Windows, default indexable values
    </td>
    <td>
     Only works on Windows, those logs have to be shipped from Windows machines to a log aggregator with a different utility
    </td>
   </tr>
   <tr>
    <td>
     <a class="link-https" href="https://docs.docker.com/engine/admin/logging/gcplogs/" rel="external nofollow" target="_blank">
      gcplogs
     </a>
    </td>
    <td>
     Simple integration with Google Compute, less infrastructure to maintain, tag support
    </td>
    <td>
     Not the most ideal for hybrid cloud configurations or on-premise installations, no
     <code>
      docker logs
     </code>
     support
    </td>
   </tr>
  </tbody>
 </table>
 <div class="mt-section" id="section_19" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
  <span id="Application_Log_Driver_Considerations">
  </span>
  <h3 id="2-7">
   Application Log Driver Considerations
  </h3>
  <p>
   Consider the following when selecting application log drivers:
  </p>
  <ul>
   <li>
    If log data is highly sensitive, then
    <strong>
     syslog
    </strong>
    and
    <strong>
     splunk
    </strong>
    are good options since they can be configured to use TLS for transporting logs.
   </li>
   <li>
    The
    <strong>
     journald
    </strong>
    log driver is great for retaining the usage of
    <code>
     docker logs
    </code>
    as well as logging Docker daemon logs. This driver allows for easier troubleshooting and log portability at the same time. Another advantage of this driver is that logs will write first locally, so that there is less reliance on logging infrastructure.
   </li>
   <li>
    If the Docker EE cluster exist solely on a single cloud provider, then
    <strong>
     awslogs
    </strong>
    or
    <strong>
     gcplogs
    </strong>
    can be used.
   </li>
   <li>
    If there's an existing Splunk installation, then use the
    <strong>
     splunk
    </strong>
    log driver.
   </li>
   <li>
    The
    <strong>
     gelf
    </strong>
    and
    <strong>
     fluentd
    </strong>
    log drivers are a good choice if there's a NoSQL database somewhere in the environment where the logs can be stored.
   </li>
   <li>
    For development or test environments, using
    <strong>
     json-file
    </strong>
    or
    <strong>
     journald
    </strong>
    could be useful where it's more useful to view a log stream rather than index and search the logs. (If
    <strong>
     json-file
    </strong>
    is used consider passing the
    <code>
     max-size
    </code>
    and
    <code>
     max-file
    </code>
    options so that logs won't fill up the filesystem.)
   </li>
  </ul>
 </div>
</div>
<div class="mt-section" id="section_20" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Logging_Driver_Setup_Walkthrough">
 </span>
 <h2 id="1-7">
  Logging Driver Setup Walkthrough
 </h2>
 <p>
  To implement system-wide logging, creating an entry in
  <code>
   /etc/docker/daemon.json
  </code>
  . For example, use the following to enable the
  <code>
   gelf
  </code>
  output plugin:
 </p>
 <pre>
<code>{
    "log-driver": "gelf",
    "log-opts": {
     "gelf-address": "udp://1.2.3.4:12201",
     "tag":"{{.ImageName}}/{{.Name}}/{{.ID}}"
    }
}
</code></pre>
 <p>
  And then restart the Docker daemon. All of the logging drivers can be configured in a similar way, by using the
  <code>
   /etc/docker/daemon.json
  </code>
  file. In the previous example using the
  <code>
   gelf
  </code>
  log driver, the
  <code>
   tag
  </code>
  field sets additional data that can be searched and indexed when logs are collected. Please refer to the documentation for each of the logging drivers to see what additional fields can be set from the log driver.
 </p>
 <p>
  Setting logs using the
  <code>
   /etc/docker/daemon.json
  </code>
  file will set the default logging behavior on a per-node basis. This can be overwritten on a per-service or a per-container level. Overwriting the default logging behavior can be useful for troubleshooting so that the logs can be viewed in real-time.
 </p>
 <p>
  If a service is created on a system where the
  <code>
   daemon.json
  </code>
  file is configured to use the
  <code>
   gelf
  </code>
  log driver, then
  <em>
   all
  </em>
  container logs running on that host will go to where the
  <code>
   gelf-address
  </code>
  config is set.
 </p>
 <p>
  If a different logging driver is preferred, for instance to view a log stream from the
  <code>
   stdout
  </code>
  of the container, then it's possible to override the default logging behavior ad-hoc.
 </p>
 <pre>
<code>docker service create \
      -–log-driver json-file --log-opt max-size=10m \
      nginx:alpine
</code></pre>
 <p>
  This can then be coupled with Docker service logs to more readily identify issues with the service.
 </p>
</div>
<div class="mt-section" id="section_21" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Docker_Service_Logs">
 </span>
 <h2 id="1-8">
  Docker Service Logs
 </h2>
 <p>
  <code>
   docker service logs
  </code>
  was introduced in version 17.05 of and version 17.06 of Docker EE. It provides a multiplexed stream of logs when a service has multiple replica tasks. By entering in
  <code>
   docker service logs &lt;service_id&gt;
  </code>
  , the logs show the originating task name in the first column and then real-time logs of each replica in the right column. For example:
 </p>
 <pre>
$ docker service create -d --name ping --replicas=3 alpine:latest ping 8.8.8.8
5x3enwyyr1re3hg1u2nogs40z

$ docker service logs ping
ping.2.n0bg40kksu8e@m00    | 64 bytes from 8.8.8.8: seq=43 ttl=43 time=24.791 ms
ping.3.pofxdol20p51@w01    | 64 bytes from 8.8.8.8: seq=44 ttl=43 time=34.161 ms
ping.1.o07dvxfx2ou2@w00    | 64 bytes from 8.8.8.8: seq=44 ttl=43 time=30.111 ms
ping.2.n0bg40kksu8e@m00    | 64 bytes from 8.8.8.8: seq=44 ttl=43 time=25.276 ms
ping.3.pofxdol20p51@w01    | 64 bytes from 8.8.8.8: seq=45 ttl=43 time=24.239 ms
ping.1.o07dvxfx2ou2@w00    | 64 bytes from 8.8.8.8: seq=45 ttl=43 time=26.403 ms
</pre>
 <p>
  This command is useful when trying to view the log output of a service that contains multiple replicas. Viewing the logs in real time, streamed across multiple replicas allows for instant understanding and troubleshooting of service issues across the entire cluster.
 </p>
</div>
<div class="mt-section" id="section_22" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="ELK_Setup">
 </span>
 <h2 id="1-9">
  ELK Setup
 </h2>
 <p>
  Please refer to the
  <a class="link-https" href="https://github.com/ahromis/swarm-elk" rel="external nofollow" target="_blank">
   swarm-elk repository
  </a>
  for information on how to send logs to ELK on a Docker Swarm. This repo contains a Docker Compose file that sets up a complete ELK stack. The repository is a good starting point for experimenting with Docker logging with ELK, but also consider high availability and a queuing system before it's used in a production manner.
 </p>
</div>
<div class="mt-section" id="section_23" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Splunk_Setup">
 </span>
 <h2 id="1-10">
  Splunk Setup
 </h2>
 <p>
  Splunk is another popular logging utility. To set up Splunk follow the steps using the Docker Compose files available in this
  <a class="link-https" href="https://github.com/ahromis/swarm-splunk" rel="external nofollow" target="_blank">
   swarm-splunk
  </a>
  repository.
 </p>
 <p>
  Each Splunk forwarder connects to the local Docker socket, so it doesn't need additional log driver configuration at the daemon level. Connecting to the local socket also allows Splunk to pull out Docker container statistics in addition to logs.
 </p>
</div>
<div class="mt-section" id="section_24" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Modernize_Traditional_Applications">
 </span>
 <h2 id="1-11">
  Modernize Traditional Applications
 </h2>
 <p>
  There's no reason to think that logging using containers is only meant for modern applications. It's possible to modernize traditional applications and still have the added benefit of modernized logging without changing any application code.
 </p>
 <p>
  Ideally, applications log to
  <code>
   stdout
  </code>
  /
  <code>
   stderr
  </code>
  , and Docker sends those logs to the configured logging destination. Sometimes certain applications are configured to log to multiple locations. How can those logs be captured without having to change any of the application source code?
 </p>
 <p>
  It's possible to create a Docker volume (or many volumes) directed at where those log files reside within the application. By leveraging Docker templating, it's possible to suffix each volume with the service task ID (an integer). Placing a suffix on the volume name prevents any collisions in regards to logging should multiple service tasks run on the same host. A global service needs to be created that runs a logging agent with directory wildcard support. Finally, additional regex can be setup via the logging utility that turns the source directory of the file, into an indexed value.
 </p>
 <p>
  The following example shows how this can be accomplished using the official
  <a class="link-https" href="https://store.docker.com/images/tomcat" rel="external nofollow" target="_blank">
   Tomcat
  </a>
  image. The official Tomcat image logs several files in
  <code>
   /usr/local/tomcat/logs
  </code>
  , much like most Java applications would do. In that path, files such as
  <code>
   catalina.2017-07-06.log
  </code>
  ,
  <code>
   host-manager.2017-07-06.log
  </code>
  ,
  <code>
   localhost.2017-07-06.log
  </code>
  ,
  <code>
   localhost_access_log.2017-07-06.txt
  </code>
  , and
  <code>
   manager.2017-07-06.log
  </code>
  can be found.
 </p>
 <ol>
  <li>
   Create a global service for the logging utility that mounts
   <code>
    /var/lib/docker/volumes:/logs/volumes
   </code>
   .
  </li>
  <li>
   Create a logging rule for the logging agent that logs using a rule similar to this generic example:
   <code>
    "/log/volumes/*/_data/*.log"
   </code>
   .
  </li>
  <li>
   <p>
    Launch a service using go based templating on the volumes:
   </p>
   <p>
    When launching the service, use these parameters:
   </p>
   <pre>
<code>docker service create \ 
-d \ 
--name prod-tomcat \ 
--label version=1.5 \ 
--label environment=prod \ 
--mount type=volume,src="{{.Task.Name}}",dst=/usr/local/tomcat/logs \ 
--replicas 3 \ 
tomcat:latest</code></pre>
   <p>
    If both replicas schedule on the same node, then two volumes containing the logs will be create on the host
    <code>
     prod-tomcat.1.oro7h0e5yow69p9yumaetor3l
    </code>
    and
    <code>
     prod-tomcat.2.ez0jpuqe2mkl6ppqnuffxqagl
    </code>
    .
   </p>
  </li>
  <li>
   As long as the logging agent supports wildcards and handles any log rotation by checking the inode (not the file), then the logs should be collected.
  </li>
  <li>
   If the application logs to multiple locations, then try to symlink the logs to a single directory or add a descriptive name to the volume. If a descriptive name is added to the volume name, then any sort of extraction will need to be updated to accommodate that change. (i.e. with a
   <code>
    grok
   </code>
   )
  </li>
  <li>
   Most loggers should collect the file path as well as the log contents. By turning the volumes where the log files reside into indexable fields it's possible to search and aggregate information from these types of applications. Here is an example that uses a
   <code>
    grok
   </code>
   pattern and creates two new indexable fields,
   <code>
    CONTAINER_NAME
   </code>
   and
   <code>
    FILENAME
   </code>
   .
   <pre>
<code>match =&gt; { "path" =&gt; "/log/volumes/%{DATA:CONTAINER_NAME}/_data/%{GREEDYDATA:FILE_NAME}" }</code></pre>
  </li>
  <li>
   The
   <code>
    CONTAINER_NAME
   </code>
   will match the output of the
   <code>
    stdout
   </code>
   stream from the container, making it easy to filter based on the container's logs.
  </li>
 </ol>
 <blockquote>
  <p>
   More information to a repository with a working example can be found in the
   <a class="link-https" href="https://github.com/ahromis/swarm-elk/tree/master/logstash" rel="external nofollow" target="_blank">
    swarm-elk repo
   </a>
   .
  </p>
 </blockquote>
</div>
<div class="mt-section" id="section_25" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Windows_Logging">
 </span>
 <h2 id="1-12">
  Windows Logging
 </h2>
 <p>
  The ETW logging driver forwards container logs as ETW events. ETW stands for Event Tracing in Windows and is the common framework for tracing applications in Windows. Each ETW event contains a message with both the log and its context information. A client can then create an ETW listener to listen to these events and forward them to a location where the logs can be collected and analyzed.
 </p>
 <p>
  To log using the ETW driver on Windows, create a service or run a container with the flag
  <code>
   --log-driver=etwlogs
  </code>
  .
 </p>
 <p>
  Here's is an example event message:
 </p>
 <pre>
<code>container_name: backstabbing_spence,
image_name: windowsservercore,
container_id: f14bb55aa862d7596b03a33251c1be7dbbec8056bbdead1da8ec5ecebbe29731,
image_id: sha256:2f9e19bd998d3565b4f345ac9aaf6e3fc555406239a4fb1b1ba879673713824b,
source: stdout,
log: Hello world!
</code></pre>
</div>
<div class="mt-section" id="section_26" mt-section-origin="Architecture/Docker_Reference_Architecture:_Docker_Logging_Design_and_Best_Practices">
 <span id="Conclusion">
 </span>
 <h2 id="1-13">
  Conclusion
 </h2>
 <p>
  Docker provides many options when it comes to logging, and it's helpful to have a logging strategy when adopting the platform. For most systems, leaving the log data on the host isn't adequate. Being able to index, search, and have a self-service platform allows for a smoother experience for both operators and developers.
 </p>
 <p>
  <em>
   Document Version: 1.0.1
  </em>
 </p>
 <p>
  <em>
   Tested on: Docker EE 17.03.2-ee5, Docker EE 17.06.1-ee
  </em>
 </p>
</div>
{% endraw %}
